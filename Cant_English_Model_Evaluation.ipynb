{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Machine Translation for Cantonese-English Language Pair\n",
        "Florence Yuen\n",
        "- Uses datasets from Tatoeba and OpenSubtitles to load Cantonese-English language pair data\n",
        "- Preprocess character-based and Jyutping romanized Cantonese data by tokenizing and doing data cleansing\n",
        "- Apply mBART-50 pre-trained multilingual NMT model\n",
        "- Compare and evaluate greedy and beam search decoding strategies\n",
        "- Save translation outputs to csv file\n",
        "- Generates BLEU scores to evaluate and compare the two decoding strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: datasets in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: sacrebleu in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: pandas in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: sentencepiece in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: pypinyin in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (0.55.0)\n",
            "Requirement already satisfied: epitran in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (1.26.0)\n",
            "Requirement already satisfied: hf_xet in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (1.1.5)\n",
            "Requirement already satisfied: protobuf in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (6.31.1)\n",
            "Requirement already satisfied: filelock in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
            "Requirement already satisfied: portalocker in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: setuptools in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from epitran) (78.1.1)\n",
            "Requirement already satisfied: panphon>=0.20 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from epitran) (0.22.2)\n",
            "Requirement already satisfied: marisa-trie in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from epitran) (1.2.1)\n",
            "Requirement already satisfied: jamo in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from epitran) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
            "Requirement already satisfied: unicodecsv in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from panphon>=0.20->epitran) (0.14.1)\n",
            "Requirement already satisfied: editdistance in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Requirement already satisfied: munkres in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from panphon>=0.20->epitran) (1.1.4)\n",
            "Requirement already satisfied: six>=1.5 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\apps\\programming\\anaconda3\\envs\\mse_446\\lib\\site-packages (from portalocker->sacrebleu) (308)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets sacrebleu pandas sentencepiece pypinyin epitran hf_xet protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pretrained mBART-50 NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e89dac8634fd43bfa9160027816c10ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83e8948f52dd4a96947a923f29d2b500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "# Import the tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer.src_lang = \"yue_Hant\"\n",
        "tokenizer.tgt_lang = \"en_XX\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset from Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "136b6b1bbd194ce8bccc6e5405e4b83f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "376b6a8763754d09802934b006b6df50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "flores.py: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset scripts are no longer supported, but found flores.py",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Open subtitles cantonese-english dataset\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# dataset = load_dataset(\"opensubtitles\", lang1=\"zh_yue\", lang2=\"en\")\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# dataset = load_dataset(\"Helsinki-NLP/open_subtitles\", lang1=\"zh_yue\", lang2=\"en\", streaming=True)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mfacebook/flores\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myue_Hant-eng\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m small_data = dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m))\n\u001b[32m     11\u001b[39m df = pd.DataFrame(small_data[\u001b[33m'\u001b[39m\u001b[33mtranslation\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\apps\\Programming\\anaconda3\\envs\\MSE_446\\Lib\\site-packages\\datasets\\load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = load_dataset_builder(\n\u001b[32m   1393\u001b[39m     path=path,\n\u001b[32m   1394\u001b[39m     name=name,\n\u001b[32m   1395\u001b[39m     data_dir=data_dir,\n\u001b[32m   1396\u001b[39m     data_files=data_files,\n\u001b[32m   1397\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   1398\u001b[39m     features=features,\n\u001b[32m   1399\u001b[39m     download_config=download_config,\n\u001b[32m   1400\u001b[39m     download_mode=download_mode,\n\u001b[32m   1401\u001b[39m     revision=revision,\n\u001b[32m   1402\u001b[39m     token=token,\n\u001b[32m   1403\u001b[39m     storage_options=storage_options,\n\u001b[32m   1404\u001b[39m     **config_kwargs,\n\u001b[32m   1405\u001b[39m )\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\apps\\Programming\\anaconda3\\envs\\MSE_446\\Lib\\site-packages\\datasets\\load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = dataset_module_factory(\n\u001b[32m   1133\u001b[39m     path,\n\u001b[32m   1134\u001b[39m     revision=revision,\n\u001b[32m   1135\u001b[39m     download_config=download_config,\n\u001b[32m   1136\u001b[39m     download_mode=download_mode,\n\u001b[32m   1137\u001b[39m     data_dir=data_dir,\n\u001b[32m   1138\u001b[39m     data_files=data_files,\n\u001b[32m   1139\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   1140\u001b[39m )\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\apps\\Programming\\anaconda3\\envs\\MSE_446\\Lib\\site-packages\\datasets\\load.py:1031\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\apps\\Programming\\anaconda3\\envs\\MSE_446\\Lib\\site-packages\\datasets\\load.py:989\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     api.hf_hub_download(\n\u001b[32m    983\u001b[39m         repo_id=path,\n\u001b[32m    984\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    987\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    988\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[31mRuntimeError\u001b[39m: Dataset scripts are no longer supported, but found flores.py"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Open subtitles cantonese-english dataset\n",
        "# dataset = load_dataset(\"opensubtitles\", lang1=\"zh_yue\", lang2=\"en\")\n",
        "# dataset = load_dataset(\"Helsinki-NLP/open_subtitles\", lang1=\"zh_yue\", lang2=\"en\", streaming=True)\n",
        "# dataset = load_dataset(\"facebook/flores\", \"yue_Hant-eng\")\n",
        "\n",
        "# small_data = dataset['train'].select(range(100))\n",
        "# df = pd.DataFrame(small_data['translation'])\n",
        "# df = df.rename(columns={'zh_yue': 'cantonese', 'en': 'english'})\n",
        "# df.head()\n",
        "\n",
        "# Tatoeba cantonese-english dataset\n",
        "# dataset = load_dataset(\"Helsinki-NLP/tatoeba_mt\", \"yue-eng\")\n",
        "# small_data = dataset['test'].select(range(100))  # Small subset\n",
        "# df = small_data.to_pandas()[[\"sourceString\", \"targetString\"]]\n",
        "# df.columns = [\"cantonese\", \"english\"]\n",
        "# df.dropna(inplace=True)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408b9b7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Manually load downloaded .en and .yue files, apply pre-processing to clean the text\n",
        "def load_parallel_corpus(cantonese_file, english_file, max_lines=None):\n",
        "    # Open the cantonese and english files\n",
        "    with open(cantonese_file, encoding='utf-8') as f_yue, open(english_file, encoding='utf-8') as f_en:\n",
        "        yue_lines = f_yue.readlines()\n",
        "        en_lines = f_en.readlines()\n",
        "\n",
        "    # Ensure that there is the same line count\n",
        "    if max_lines:\n",
        "        yue_lines = yue_lines[:max_lines]\n",
        "        en_lines = en_lines[:max_lines]\n",
        "\n",
        "    assert len(yue_lines) == len(en_lines), \"Line count mismatch!\"\n",
        "\n",
        "    #Apply preprocessing to clean the text \n",
        "    def clean_text(text):\n",
        "        # Remove brackets\n",
        "        text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "        text = re.sub(r'\\([^\\)]*\\)', '', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    # Clean each line of text for both cantonese and english files\n",
        "    yue_lines = [clean_text(line) for line in yue_lines]\n",
        "    en_lines = [clean_text(line) for line in en_lines]\n",
        "\n",
        "    # Build the DataFrame and drop empty lines\n",
        "    df = pd.DataFrame({'cantonese': yue_lines, 'english': en_lines})\n",
        "    df = df[(df['cantonese'] != '') & (df['english'] != '')].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# Load Tatoeba dataset\n",
        "df = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Translation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e51a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # for progress bar\n",
        "\n",
        "# Translation function with tydm to add a loading progress bar for insights\n",
        "def translate(texts, beam=1, batch_size=16):\n",
        "    translations = []\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    \n",
        "    # Devide into batches so that the progress/ percentage is shown too\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate & Compare Decoding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/63 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   2%|▏         | 1/63 [00:19<19:58, 19.34s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   3%|▎         | 2/63 [00:54<28:55, 28.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   5%|▍         | 3/63 [01:20<27:39, 27.65s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   6%|▋         | 4/63 [01:43<25:13, 25.66s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   8%|▊         | 5/63 [02:07<24:23, 25.23s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  10%|▉         | 6/63 [02:24<21:04, 22.19s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  11%|█         | 7/63 [02:43<19:56, 21.37s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  13%|█▎        | 8/63 [03:07<20:19, 22.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  14%|█▍        | 9/63 [03:53<26:27, 29.40s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  16%|█▌        | 10/63 [04:17<24:34, 27.83s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  17%|█▋        | 11/63 [04:42<23:24, 27.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  19%|█▉        | 12/63 [05:01<20:48, 24.47s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  21%|██        | 13/63 [05:19<18:56, 22.73s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  22%|██▏       | 14/63 [05:43<18:39, 22.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  24%|██▍       | 15/63 [06:09<19:09, 23.95s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  25%|██▌       | 16/63 [06:30<17:57, 22.92s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  27%|██▋       | 17/63 [06:51<17:16, 22.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  29%|██▊       | 18/63 [07:09<15:54, 21.21s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  30%|███       | 19/63 [07:29<15:11, 20.72s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  32%|███▏      | 20/63 [07:48<14:31, 20.26s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  33%|███▎      | 21/63 [08:11<14:44, 21.05s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  35%|███▍      | 22/63 [08:37<15:26, 22.60s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  37%|███▋      | 23/63 [08:57<14:25, 21.63s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  38%|███▊      | 24/63 [09:25<15:25, 23.73s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  40%|███▉      | 25/63 [09:58<16:43, 26.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  41%|████▏     | 26/63 [10:15<14:35, 23.65s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  43%|████▎     | 27/63 [10:35<13:29, 22.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  44%|████▍     | 28/63 [10:56<12:48, 21.94s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  46%|████▌     | 29/63 [11:15<11:55, 21.06s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  48%|████▊     | 30/63 [11:47<13:27, 24.47s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  49%|████▉     | 31/63 [12:04<11:51, 22.23s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  51%|█████     | 32/63 [12:22<10:47, 20.88s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  52%|█████▏    | 33/63 [12:45<10:50, 21.67s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  54%|█████▍    | 34/63 [13:07<10:27, 21.65s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  56%|█████▌    | 35/63 [13:24<09:25, 20.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  57%|█████▋    | 36/63 [13:48<09:40, 21.49s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  59%|█████▊    | 37/63 [14:05<08:42, 20.10s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  60%|██████    | 38/63 [14:22<08:02, 19.32s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  62%|██████▏   | 39/63 [14:45<08:04, 20.20s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  63%|██████▎   | 40/63 [15:16<09:02, 23.58s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  65%|██████▌   | 41/63 [15:46<09:16, 25.32s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  67%|██████▋   | 42/63 [16:13<09:04, 25.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  68%|██████▊   | 43/63 [16:39<08:37, 25.89s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  70%|██████▉   | 44/63 [16:53<07:03, 22.27s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  71%|███████▏  | 45/63 [17:14<06:37, 22.08s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  73%|███████▎  | 46/63 [17:30<05:43, 20.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  75%|███████▍  | 47/63 [17:50<05:23, 20.22s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  76%|███████▌  | 48/63 [18:11<05:05, 20.34s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  78%|███████▊  | 49/63 [18:26<04:23, 18.82s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  79%|███████▉  | 50/63 [18:41<03:49, 17.64s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  81%|████████  | 51/63 [19:01<03:40, 18.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  83%|████████▎ | 52/63 [19:25<03:41, 20.14s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  84%|████████▍ | 53/63 [19:47<03:27, 20.71s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  86%|████████▌ | 54/63 [20:05<02:58, 19.83s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  87%|████████▋ | 55/63 [20:23<02:32, 19.07s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  89%|████████▉ | 56/63 [20:39<02:08, 18.33s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  90%|█████████ | 57/63 [20:59<01:53, 18.87s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  92%|█████████▏| 58/63 [21:15<01:30, 18.05s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  94%|█████████▎| 59/63 [21:35<01:14, 18.59s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  95%|█████████▌| 60/63 [21:56<00:57, 19.20s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  97%|█████████▋| 61/63 [22:17<00:39, 19.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  98%|█████████▊| 62/63 [22:34<00:18, 18.94s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating: 100%|██████████| 63/63 [22:52<00:00, 21.78s/it]\n",
            "Translating: 100%|██████████| 63/63 [3:36:19<00:00, 206.02s/it]    \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I'm going to go to the temple.</td>\n",
              "      <td>I'm going to go to the monastery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I'm going to say I will give up a fight and go...</td>\n",
              "      <td>I said I was going to do a series I would give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I'm not a big fan of mysterious characters.</td>\n",
              "      <td>I don't think I've ever heard of a mysterious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>I'm not sure if I can do it, but I have a good...</td>\n",
              "      <td>I'm not sure if we're going to live together, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I'm only a single one.</td>\n",
              "      <td>I'm the only one who can wait.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                     I'm going to go to the temple.   \n",
              "1  I'm going to say I will give up a fight and go...   \n",
              "2        I'm not a big fan of mysterious characters.   \n",
              "3  I'm not sure if I can do it, but I have a good...   \n",
              "4                             I'm only a single one.   \n",
              "\n",
              "                                                beam  \n",
              "0                  I'm going to go to the monastery.  \n",
              "1  I said I was going to do a series I would give...  \n",
              "2  I don't think I've ever heard of a mysterious ...  \n",
              "3  I'm not sure if we're going to live together, ...  \n",
              "4                     I'm the only one who can wait.  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run translations for greedy and beam search algorithm\n",
        "df['greedy'] = translate(df['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "df['beam'] = translate(df['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate algorithms using BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy BLEU: 7.94\n",
            "Beam BLEU: 9.21\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Define hypotheses for greedy and beam search (as string lists)\n",
        "greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "\n",
        "# Define references as string lists\n",
        "references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "# Calculate the bleu score\n",
        "greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "# Display bleu score\n",
        "print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "print(f\"Beam BLEU: {beam_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Translation Comaprison Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I'm going to go to the temple.</td>\n",
              "      <td>I'm going to go to the monastery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I'm going to say I will give up a fight and go...</td>\n",
              "      <td>I said I was going to do a series I would give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I'm not a big fan of mysterious characters.</td>\n",
              "      <td>I don't think I've ever heard of a mysterious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>I'm not sure if I can do it, but I have a good...</td>\n",
              "      <td>I'm not sure if we're going to live together, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I'm only a single one.</td>\n",
              "      <td>I'm the only one who can wait.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                     I'm going to go to the temple.   \n",
              "1  I'm going to say I will give up a fight and go...   \n",
              "2        I'm not a big fan of mysterious characters.   \n",
              "3  I'm not sure if I can do it, but I have a good...   \n",
              "4                             I'm only a single one.   \n",
              "\n",
              "                                                beam  \n",
              "0                  I'm going to go to the monastery.  \n",
              "1  I said I was going to do a series I would give...  \n",
              "2  I don't think I've ever heard of a mysterious ...  \n",
              "3  I'm not sure if we're going to live together, ...  \n",
              "4                     I'm the only one who can wait.  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.to_csv(\"translation_comparison_results.csv\", index=False)\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MSE_446",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
