{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Machine Translation for Cantonese-English Language Pair\n",
        "Florence Yuen\n",
        "- Uses datasets from Tatoeba and OpenSubtitles to load Cantonese-English language pair data\n",
        "- Preprocess character-based and Jyutping romanized Cantonese data by tokenizing and doing data cleansing\n",
        "- Apply mBART-50 pre-trained multilingual NMT model\n",
        "- Compare and evaluate greedy and beam search decoding strategies\n",
        "- Save translation outputs to csv file\n",
        "- Generates BLEU scores to evaluate and compare the two decoding strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: datasets in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: sacrebleu in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: pandas in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: sentencepiece in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: pypinyin in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (0.55.0)\n",
            "Requirement already satisfied: epitran in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (1.26.0)\n",
            "Requirement already satisfied: hf_xet in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (1.1.5)\n",
            "Requirement already satisfied: protobuf in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (6.31.1)\n",
            "Requirement already satisfied: ipywidgets in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (8.1.7)\n",
            "Requirement already satisfied: filelock in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: portalocker in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (6.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: setuptools in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (65.5.0)\n",
            "Requirement already satisfied: panphon>=0.20 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (0.22.2)\n",
            "Requirement already satisfied: marisa-trie in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (1.2.1)\n",
            "Requirement already satisfied: jamo in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (0.4.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (9.4.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: decorator in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: unicodecsv in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (0.14.1)\n",
            "Requirement already satisfied: editdistance in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Requirement already satisfied: munkres in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (1.1.4)\n",
            "Requirement already satisfied: six>=1.5 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (311)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install -r requirements.txt\n",
        "# !pip install transformers datasets sacrebleu pandas sentencepiece pypinyin epitran hf_xet protobuf ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pretrained mBART-50 NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1df670ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # for progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79483bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "# Import the tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer.src_lang = \"yue_Hant\"\n",
        "tokenizer.tgt_lang = \"en_XX\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7e979497",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3254fb92a5f47ee8f465637d2eabc0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FY\\.cache\\huggingface\\hub\\models--facebook--m2m100_418M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e5bd1271a164e0faf01a724350b97f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a17e620c1d0b4a8f859a6a2f0590c906",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5283910885744b59e1b96825c5053ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c65f9f4046340e09ed2bf3ab466b752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a3a194ef5714ae184255cd4ae7b5328",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89fe394d795943f8aed205f259553d6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9f4e7cb6a14affb0ddbab0491482df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "model_name_m2m = \"facebook/m2m100_418M\"\n",
        "tokenizer_m2m = M2M100Tokenizer.from_pretrained(model_name_m2m)\n",
        "model_m2m = M2M100ForConditionalGeneration.from_pretrained(model_name_m2m).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9402d6df",
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Helsinki-NLP/opus-mt-yue-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/Helsinki-NLP/opus-mt-yue-en/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:476\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
            "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-688b7c5d-444991c35dfb9ec731643765;79b0df25-eba6-492a-8036-fc9b49ab2169)\n\nRepository Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-yue-en/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarianTokenizer, MarianMTModel\n\u001b[32m      4\u001b[39m model_name_mar = \u001b[33m\"\u001b[39m\u001b[33mHelsinki-NLP/opus-mt-yue-en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer_mar = \u001b[43mMarianTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_mar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model_mar = MarianMTModel.from_pretrained(model_name_mar).to(device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1911\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1908\u001b[39m fast_tokenizer_file = FULL_TOKENIZER_FILE\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;66;03m# Re-raise any error raised by cached_file in order to get a helpful error message\u001b[39;00m\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:318\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    261\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    262\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    263\u001b[39m     **kwargs,\n\u001b[32m    264\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    267\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:508\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    509\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    516\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    517\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    518\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    519\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mOSError\u001b[39m: Helsinki-NLP/opus-mt-yue-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "# Load MarianMT Model\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "model_name_mar = \"Helsinki-NLP/opus-mt-yue-en\"\n",
        "tokenizer_mar = MarianTokenizer.from_pretrained(model_name_mar)\n",
        "model_mar = MarianMTModel.from_pretrained(model_name_mar).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset from Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "408b9b7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Manually load downloaded .en and .yue files, apply pre-processing to clean the text\n",
        "def load_parallel_corpus(cantonese_file, english_file, max_lines=None):\n",
        "    # Open the cantonese and english files\n",
        "    with open(cantonese_file, encoding='utf-8') as f_yue, open(english_file, encoding='utf-8') as f_en:\n",
        "        yue_lines = f_yue.readlines()\n",
        "        en_lines = f_en.readlines()\n",
        "\n",
        "    # Ensure that there is the same line count\n",
        "    if max_lines:\n",
        "        yue_lines = yue_lines[:max_lines]\n",
        "        en_lines = en_lines[:max_lines]\n",
        "\n",
        "    assert len(yue_lines) == len(en_lines), \"Line count mismatch!\"\n",
        "\n",
        "    #Apply preprocessing to clean the text \n",
        "    def clean_text(text):\n",
        "        # Remove brackets\n",
        "        text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "        text = re.sub(r'\\([^\\)]*\\)', '', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    # Clean each line of text for both cantonese and english files\n",
        "    yue_lines = [clean_text(line) for line in yue_lines]\n",
        "    en_lines = [clean_text(line) for line in en_lines]\n",
        "\n",
        "    # Build the DataFrame and drop empty lines\n",
        "    df = pd.DataFrame({'cantonese': yue_lines, 'english': en_lines})\n",
        "    df = df[(df['cantonese'] != '') & (df['english'] != '')].reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06324449",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0                                       I have to go to sleep.\n",
            "1         I may give up soon and just nap instead.\n",
            "2            I always liked mysterious characters more.\n",
            "3     Even though he apologized, I'm still furious.\n",
            "4                                              I can only wait.\n"
          ]
        }
      ],
      "source": [
        "# # Load Tatoeba dataset\n",
        "df = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5185f99b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0                                                            Look.\n",
            "1                                             - That kills me. -\n",
            "2  ,  Come on, we'll fix him. Let's give him the old...\n",
            "3        ?                            Won't you ever grow up?\n",
            "4                                               - What? - Yoo-hoo!\n"
          ]
        }
      ],
      "source": [
        "# Load 1000 lines from OpenSubtitles dataset (downloaded from Opus)\n",
        "df_Open_Subtitles = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_Open_Subtitles.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Translation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "51e51a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation function with tydm to add a loading progress bar for insights using mBART50 model\n",
        "def translate(texts, beam=1, batch_size=16):\n",
        "    translations = []\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    \n",
        "    # Devide into batches so that the progress/ percentage is shown too\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n",
        "\n",
        "# Translation function with model and tokenizer being passed in\n",
        "def translate_select_model(texts, tokenizer, model, beam=1, batch_size=16, device=\"cpu\"):\n",
        "    translations = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate & Compare Decoding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dcba1fc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translations for greedy and beam search algorithm using Tatoeba dataset\n",
        "# df['greedy'] = translate(df['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "# df['beam'] = translate(df['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c2898399",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0                                       I have to go to sleep.\n",
            "1         I may give up soon and just nap instead.\n",
            "2            I always liked mysterious characters more.\n",
            "3     Even though he apologized, I'm still furious.\n",
            "4                                              I can only wait.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/63 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   2%|         | 1/63 [00:20<21:07, 20.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   3%|         | 2/63 [00:38<19:17, 18.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   5%|         | 3/63 [00:53<17:15, 17.26s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   6%|         | 4/63 [01:04<14:36, 14.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   8%|         | 5/63 [01:20<14:43, 15.23s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  10%|         | 6/63 [01:31<13:02, 13.73s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  11%|         | 7/63 [01:42<11:55, 12.77s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  13%|        | 8/63 [01:54<11:27, 12.50s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  14%|        | 9/63 [02:04<10:43, 11.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  16%|        | 10/63 [02:24<12:43, 14.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  17%|        | 11/63 [02:36<11:38, 13.44s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  19%|        | 12/63 [02:57<13:31, 15.90s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  21%|        | 13/63 [03:11<12:48, 15.38s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  22%|       | 14/63 [03:22<11:27, 14.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  24%|       | 15/63 [03:40<12:13, 15.28s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  25%|       | 16/63 [03:54<11:35, 14.79s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  27%|       | 17/63 [04:08<11:08, 14.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  29%|       | 18/63 [04:22<10:42, 14.29s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  30%|       | 19/63 [04:31<09:21, 12.76s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  32%|      | 20/63 [04:43<08:59, 12.55s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  33%|      | 21/63 [04:59<09:32, 13.64s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  35%|      | 22/63 [05:16<10:03, 14.72s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  37%|      | 23/63 [05:29<09:20, 14.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  38%|      | 24/63 [05:58<11:59, 18.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  40%|      | 25/63 [06:30<14:21, 22.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  41%|     | 26/63 [06:45<12:34, 20.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  43%|     | 27/63 [07:02<11:31, 19.22s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  44%|     | 28/63 [07:14<10:03, 17.25s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  46%|     | 29/63 [07:24<08:30, 15.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  48%|     | 30/63 [07:39<08:09, 14.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  49%|     | 31/63 [07:49<07:10, 13.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  51%|     | 32/63 [08:02<06:59, 13.52s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  52%|    | 33/63 [08:11<05:57, 11.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  54%|    | 34/63 [08:24<05:55, 12.27s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  56%|    | 35/63 [08:40<06:19, 13.56s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  57%|    | 36/63 [08:50<05:35, 12.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  59%|    | 37/63 [08:59<04:54, 11.34s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  60%|    | 38/63 [09:13<05:04, 12.16s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  62%|   | 39/63 [09:27<05:04, 12.71s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  63%|   | 40/63 [09:44<05:19, 13.89s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  65%|   | 41/63 [09:55<04:47, 13.08s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  67%|   | 42/63 [10:06<04:26, 12.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  68%|   | 43/63 [10:42<06:30, 19.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  70%|   | 44/63 [11:00<06:04, 19.21s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  71%|  | 45/63 [11:13<05:08, 17.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  73%|  | 46/63 [11:23<04:14, 14.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  75%|  | 47/63 [11:38<04:01, 15.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  76%|  | 48/63 [11:47<03:17, 13.15s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  78%|  | 49/63 [11:56<02:46, 11.87s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  79%|  | 50/63 [12:02<02:11, 10.09s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  81%|  | 51/63 [12:16<02:18, 11.54s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  83%| | 52/63 [12:35<02:29, 13.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  84%| | 53/63 [12:45<02:04, 12.44s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  86%| | 54/63 [12:56<01:49, 12.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  87%| | 55/63 [13:03<01:23, 10.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  89%| | 56/63 [13:15<01:17, 11.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  90%| | 57/63 [13:36<01:24, 14.05s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  92%|| 58/63 [13:46<01:04, 12.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  94%|| 59/63 [13:56<00:48, 12.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  95%|| 60/63 [14:03<00:31, 10.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  97%|| 61/63 [14:11<00:19,  9.55s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  98%|| 62/63 [14:22<00:10, 10.03s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating: 100%|| 63/63 [14:29<00:00, 13.81s/it]\n",
            "Translating: 100%|| 63/63 [38:20<00:00, 36.51s/it] \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>   .</td>\n",
              "      <td>iiiiii iiii.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td> ,    , ...</td>\n",
              "      <td>-,      ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>     .</td>\n",
              "      <td>Je nai pas lintention de connatre les perso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>      .</td>\n",
              "      <td>-,     ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>   .</td>\n",
              "      <td>Je nai qu attendre.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0                                       I have to go to sleep.   \n",
              "1         I may give up soon and just nap instead.   \n",
              "2            I always liked mysterious characters more.   \n",
              "3     Even though he apologized, I'm still furious.   \n",
              "4                                              I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                              .   \n",
              "1   ,    , ...   \n",
              "2             .   \n",
              "3              .   \n",
              "4                                .   \n",
              "\n",
              "                                                beam  \n",
              "0                           iiiiii iiii.  \n",
              "1  -,      ...  \n",
              "2  Je nai pas lintention de connatre les perso...  \n",
              "3  -,     ...  \n",
              "4                             Je nai qu attendre.  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run using M2M\n",
        "df_m2m = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df_m2m.head())\n",
        "\n",
        "df_m2m['greedy'] = translate_select_model(df_m2m['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)\n",
        "df_m2m['beam'] = translate_select_model(df_m2m['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)\n",
        "df_m2m.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5d20159d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "from sacrebleu import corpus_chrf\n",
        "\n",
        "# Function to loop over all  dataframes defined and print out BLEU and CHRF scores\n",
        "def compute_scores(df, name=\"\"):\n",
        "    # Prepare hypothesis and reference lists\n",
        "    greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "    beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "    references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "    beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "    greedy_chrf = corpus_chrf(greedy_hypotheses, references).score\n",
        "    beam_chrf = corpus_chrf(beam_hypotheses, references).score\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{name} BLEU Scores:\")\n",
        "    print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "    print(f\"Beam BLEU:   {beam_bleu:.2f}\")\n",
        "    \n",
        "    print(f\"\\n{name} CHRF Scores:\")\n",
        "    print(f\"Greedy CHRF: {greedy_chrf:.2f}\")\n",
        "    print(f\"BEAM CHRF: {beam_chrf:.2f}\")\n",
        "    \n",
        "    return greedy_bleu, beam_bleu, greedy_chrf, beam_chrf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e3d34183",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating name: m2m\n",
            "\n",
            "m2m BLEU Scores:\n",
            "Greedy BLEU: 0.07\n",
            "Beam BLEU:   0.07\n",
            "\n",
            "m2m CHRF Scores:\n",
            "Greedy CHRF: 3.96\n",
            "BEAM CHRF: 5.22\n"
          ]
        }
      ],
      "source": [
        "results_m2m = {}\n",
        "name = \"m2m\"\n",
        "print(f\"Evaluating name: {name}\")\n",
        "greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_m2m, name)\n",
        "results_m2m[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "df_m2m.to_csv(f\"trans_res_{name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0027b2b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/62 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "# Run using M2M on OpenSubtitles dataset\n",
        "df_Open_Subtitles['greedy'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)\n",
        "df_Open_Subtitles['beam'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9e89a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translations for greedy and beam search algorithm using OpenSubtitles dataset\n",
        "df_OS_mBART = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_mBART.head())\n",
        "df_OS_mBART['greedy'] = translate(df_OS_mBART['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "df_OS_mBART['beam'] = translate(df_OS_mBART['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "df_OS_mBART.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1f3015",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Run using MarianMT on Tatoeba dataset\n",
        "# df_Tatoeba = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "# print(df_Tatoeba.head())\n",
        "\n",
        "# df_Tatoeba['greedy'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "# df_Tatoeba['beam'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)\n",
        "# df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dc650a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0                                                            Look.\n",
            "1                                             - That kills me. -\n",
            "2  ,  Come on, we'll fix him. Let's give him the old...\n",
            "3        ?                            Won't you ever grow up?\n",
            "4                                               - What? - Yoo-hoo!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer_mar' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df_OS_Mar = load_parallel_corpus(\u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.en\u001b[39m\u001b[33m'\u001b[39m, max_lines=\u001b[32m1000\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_OS_Mar.head())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mgreedy\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), \u001b[43mtokenizer_mar\u001b[49m, model_mar, beam=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n\u001b[32m      6\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), tokenizer_mar, model_mar, beam=\u001b[32m5\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n",
            "\u001b[31mNameError\u001b[39m: name 'tokenizer_mar' is not defined"
          ]
        }
      ],
      "source": [
        "# Run using MarianMT on OpenSubtitles dataset\n",
        "df_OS_Mar = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_Mar.head())\n",
        "\n",
        "df_OS_Mar['greedy'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "df_OS_Mar['beam'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate algorithms using BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy BLEU: 7.94\n",
            "Beam BLEU: 9.21\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Define hypotheses for greedy and beam search (as string lists)\n",
        "greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "\n",
        "# Define references as string lists\n",
        "references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "# Calculate the bleu score\n",
        "greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "# Display bleu score\n",
        "print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "print(f\"Beam BLEU: {beam_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684d22c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of (name, dataframe) pairs\n",
        "dataframes = [\n",
        "    # (\"Tatoeba + mBART\", df),\n",
        "    (\"Open Subtitles_mBART\", df_OS_mBART),\n",
        "    (\"Tatoeba_m2m\", df_m2m),\n",
        "    (\"Open Subtitles_m2m\", df_Open_Subtitles),\n",
        "]\n",
        "\n",
        "# Compute BLEU and CHRF scores for each DataFrame\n",
        "results = {}\n",
        "\n",
        "# Calculate BLEU and CHRF scores for each of the data frames and save to list\n",
        "for name, df_ in dataframes:\n",
        "    try:\n",
        "        print(f\"Evaluating name: {name}\")\n",
        "        greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_, name)\n",
        "        results[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "        df_.to_csv(f\"trans_res_{name}.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Translation Comaprison Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I'm going to go to the temple.</td>\n",
              "      <td>I'm going to go to the monastery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I'm going to say I will give up a fight and go...</td>\n",
              "      <td>I said I was going to do a series I would give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I'm not a big fan of mysterious characters.</td>\n",
              "      <td>I don't think I've ever heard of a mysterious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>I'm not sure if I can do it, but I have a good...</td>\n",
              "      <td>I'm not sure if we're going to live together, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I'm only a single one.</td>\n",
              "      <td>I'm the only one who can wait.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0                                       I have to go to sleep.   \n",
              "1         I may give up soon and just nap instead.   \n",
              "2            I always liked mysterious characters more.   \n",
              "3     Even though he apologized, I'm still furious.   \n",
              "4                                              I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                     I'm going to go to the temple.   \n",
              "1  I'm going to say I will give up a fight and go...   \n",
              "2        I'm not a big fan of mysterious characters.   \n",
              "3  I'm not sure if I can do it, but I have a good...   \n",
              "4                             I'm only a single one.   \n",
              "\n",
              "                                                beam  \n",
              "0                  I'm going to go to the monastery.  \n",
              "1  I said I was going to do a series I would give...  \n",
              "2  I don't think I've ever heard of a mysterious ...  \n",
              "3  I'm not sure if we're going to live together, ...  \n",
              "4                     I'm the only one who can wait.  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.to_csv(\"trans_res_Tatoeba_mBart.csv\", index=False)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459848fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_Open_Subtitles.to_csv(\"trans_res_OpenSubtitles_mBart.csv\", index=False)\n",
        "# df_Open_Subtitles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ded1e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_Tatoeba.to_csv(\"trans_res_Tatoeba_Marian.csv\", index=False)\n",
        "df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416e67cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_OS_Mar.to_csv(\"trans_res_OpenSubtitles_Marian.csv\", index=False)\n",
        "df_OS_Mar.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
