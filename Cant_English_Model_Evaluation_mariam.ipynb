{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Machine Translation for Cantonese-English Language Pair\n",
        "Florence Yuen\n",
        "- Uses datasets from Tatoeba and OpenSubtitles to load Cantonese-English language pair data\n",
        "- Preprocess character-based and Jyutping romanized Cantonese data by tokenizing and doing data cleansing\n",
        "- Apply mBART-50 pre-trained multilingual NMT model\n",
        "- Compare and evaluate greedy and beam search decoding strategies\n",
        "- Save translation outputs to csv file\n",
        "- Generates BLEU scores to evaluate and compare the two decoding strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: datasets in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: sacrebleu in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: pandas in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: sentencepiece in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: pypinyin in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (0.55.0)\n",
            "Requirement already satisfied: epitran in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (1.26.0)\n",
            "Requirement already satisfied: hf_xet in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (1.1.5)\n",
            "Requirement already satisfied: protobuf in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (6.31.1)\n",
            "Requirement already satisfied: ipywidgets in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (8.1.7)\n",
            "Requirement already satisfied: filelock in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: portalocker in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (6.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: setuptools in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (65.5.0)\n",
            "Requirement already satisfied: panphon>=0.20 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (0.22.2)\n",
            "Requirement already satisfied: marisa-trie in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (1.2.1)\n",
            "Requirement already satisfied: jamo in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (0.4.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (9.4.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: decorator in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: unicodecsv in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (0.14.1)\n",
            "Requirement already satisfied: editdistance in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Requirement already satisfied: munkres in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (1.1.4)\n",
            "Requirement already satisfied: six>=1.5 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (311)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install -r requirements.txt\n",
        "# !pip install transformers datasets sacrebleu pandas sentencepiece pypinyin epitran hf_xet protobuf ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pretrained mBART-50 NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1df670ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # for progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79483bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "# Import the tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer.src_lang = \"yue_Hant\"\n",
        "tokenizer.tgt_lang = \"en_XX\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7e979497",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3254fb92a5f47ee8f465637d2eabc0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FY\\.cache\\huggingface\\hub\\models--facebook--m2m100_418M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e5bd1271a164e0faf01a724350b97f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a17e620c1d0b4a8f859a6a2f0590c906",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5283910885744b59e1b96825c5053ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c65f9f4046340e09ed2bf3ab466b752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a3a194ef5714ae184255cd4ae7b5328",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89fe394d795943f8aed205f259553d6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9f4e7cb6a14affb0ddbab0491482df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "model_name_m2m = \"facebook/m2m100_418M\"\n",
        "tokenizer_m2m = M2M100Tokenizer.from_pretrained(model_name_m2m)\n",
        "model_m2m = M2M100ForConditionalGeneration.from_pretrained(model_name_m2m).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9402d6df",
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Helsinki-NLP/opus-mt-yue-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/Helsinki-NLP/opus-mt-yue-en/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:476\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
            "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-688b7c5d-444991c35dfb9ec731643765;79b0df25-eba6-492a-8036-fc9b49ab2169)\n\nRepository Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-yue-en/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarianTokenizer, MarianMTModel\n\u001b[32m      4\u001b[39m model_name_mar = \u001b[33m\"\u001b[39m\u001b[33mHelsinki-NLP/opus-mt-yue-en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer_mar = \u001b[43mMarianTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_mar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model_mar = MarianMTModel.from_pretrained(model_name_mar).to(device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1911\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1908\u001b[39m fast_tokenizer_file = FULL_TOKENIZER_FILE\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;66;03m# Re-raise any error raised by cached_file in order to get a helpful error message\u001b[39;00m\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:318\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    261\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    262\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    263\u001b[39m     **kwargs,\n\u001b[32m    264\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    267\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:508\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    509\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    516\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    517\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    518\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    519\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mOSError\u001b[39m: Helsinki-NLP/opus-mt-yue-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "# Load MarianMT Model\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "model_name_mar = \"Helsinki-NLP/opus-mt-yue-en\"\n",
        "tokenizer_mar = MarianTokenizer.from_pretrained(model_name_mar)\n",
        "model_mar = MarianMTModel.from_pretrained(model_name_mar).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset from Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "408b9b7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Manually load downloaded .en and .yue files, apply pre-processing to clean the text\n",
        "def load_parallel_corpus(cantonese_file, english_file, max_lines=None):\n",
        "    # Open the cantonese and english files\n",
        "    with open(cantonese_file, encoding='utf-8') as f_yue, open(english_file, encoding='utf-8') as f_en:\n",
        "        yue_lines = f_yue.readlines()\n",
        "        en_lines = f_en.readlines()\n",
        "\n",
        "    # Ensure that there is the same line count\n",
        "    if max_lines:\n",
        "        yue_lines = yue_lines[:max_lines]\n",
        "        en_lines = en_lines[:max_lines]\n",
        "\n",
        "    assert len(yue_lines) == len(en_lines), \"Line count mismatch!\"\n",
        "\n",
        "    #Apply preprocessing to clean the text \n",
        "    def clean_text(text):\n",
        "        # Remove brackets\n",
        "        text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "        text = re.sub(r'\\([^\\)]*\\)', '', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    # Clean each line of text for both cantonese and english files\n",
        "    yue_lines = [clean_text(line) for line in yue_lines]\n",
        "    en_lines = [clean_text(line) for line in en_lines]\n",
        "\n",
        "    # Build the DataFrame and drop empty lines\n",
        "    df = pd.DataFrame({'cantonese': yue_lines, 'english': en_lines})\n",
        "    df = df[(df['cantonese'] != '') & (df['english'] != '')].reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06324449",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        }
      ],
      "source": [
        "# # Load Tatoeba dataset\n",
        "df = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5185f99b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        }
      ],
      "source": [
        "# Load 1000 lines from OpenSubtitles dataset (downloaded from Opus)\n",
        "df_Open_Subtitles = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_Open_Subtitles.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Translation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "51e51a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation function with tydm to add a loading progress bar for insights using mBART50 model\n",
        "def translate(texts, beam=1, batch_size=16):\n",
        "    translations = []\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    \n",
        "    # Devide into batches so that the progress/ percentage is shown too\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n",
        "\n",
        "# Translation function with model and tokenizer being passed in\n",
        "def translate_select_model(texts, tokenizer, model, beam=1, batch_size=16, device=\"cpu\"):\n",
        "    translations = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate & Compare Decoding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dcba1fc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translations for greedy and beam search algorithm using Tatoeba dataset\n",
        "# df['greedy'] = translate(df['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "# df['beam'] = translate(df['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c2898399",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/63 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   2%|▏         | 1/63 [00:20<21:07, 20.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   3%|▎         | 2/63 [00:38<19:17, 18.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   5%|▍         | 3/63 [00:53<17:15, 17.26s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   6%|▋         | 4/63 [01:04<14:36, 14.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   8%|▊         | 5/63 [01:20<14:43, 15.23s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  10%|▉         | 6/63 [01:31<13:02, 13.73s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  11%|█         | 7/63 [01:42<11:55, 12.77s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  13%|█▎        | 8/63 [01:54<11:27, 12.50s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  14%|█▍        | 9/63 [02:04<10:43, 11.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  16%|█▌        | 10/63 [02:24<12:43, 14.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  17%|█▋        | 11/63 [02:36<11:38, 13.44s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  19%|█▉        | 12/63 [02:57<13:31, 15.90s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  21%|██        | 13/63 [03:11<12:48, 15.38s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  22%|██▏       | 14/63 [03:22<11:27, 14.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  24%|██▍       | 15/63 [03:40<12:13, 15.28s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  25%|██▌       | 16/63 [03:54<11:35, 14.79s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  27%|██▋       | 17/63 [04:08<11:08, 14.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  29%|██▊       | 18/63 [04:22<10:42, 14.29s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  30%|███       | 19/63 [04:31<09:21, 12.76s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  32%|███▏      | 20/63 [04:43<08:59, 12.55s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  33%|███▎      | 21/63 [04:59<09:32, 13.64s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  35%|███▍      | 22/63 [05:16<10:03, 14.72s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  37%|███▋      | 23/63 [05:29<09:20, 14.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  38%|███▊      | 24/63 [05:58<11:59, 18.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  40%|███▉      | 25/63 [06:30<14:21, 22.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  41%|████▏     | 26/63 [06:45<12:34, 20.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  43%|████▎     | 27/63 [07:02<11:31, 19.22s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  44%|████▍     | 28/63 [07:14<10:03, 17.25s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  46%|████▌     | 29/63 [07:24<08:30, 15.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  48%|████▊     | 30/63 [07:39<08:09, 14.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  49%|████▉     | 31/63 [07:49<07:10, 13.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  51%|█████     | 32/63 [08:02<06:59, 13.52s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  52%|█████▏    | 33/63 [08:11<05:57, 11.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  54%|█████▍    | 34/63 [08:24<05:55, 12.27s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  56%|█████▌    | 35/63 [08:40<06:19, 13.56s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  57%|█████▋    | 36/63 [08:50<05:35, 12.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  59%|█████▊    | 37/63 [08:59<04:54, 11.34s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  60%|██████    | 38/63 [09:13<05:04, 12.16s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  62%|██████▏   | 39/63 [09:27<05:04, 12.71s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  63%|██████▎   | 40/63 [09:44<05:19, 13.89s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  65%|██████▌   | 41/63 [09:55<04:47, 13.08s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  67%|██████▋   | 42/63 [10:06<04:26, 12.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  68%|██████▊   | 43/63 [10:42<06:30, 19.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  70%|██████▉   | 44/63 [11:00<06:04, 19.21s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  71%|███████▏  | 45/63 [11:13<05:08, 17.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  73%|███████▎  | 46/63 [11:23<04:14, 14.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  75%|███████▍  | 47/63 [11:38<04:01, 15.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  76%|███████▌  | 48/63 [11:47<03:17, 13.15s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  78%|███████▊  | 49/63 [11:56<02:46, 11.87s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  79%|███████▉  | 50/63 [12:02<02:11, 10.09s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  81%|████████  | 51/63 [12:16<02:18, 11.54s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  83%|████████▎ | 52/63 [12:35<02:29, 13.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  84%|████████▍ | 53/63 [12:45<02:04, 12.44s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  86%|████████▌ | 54/63 [12:56<01:49, 12.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  87%|████████▋ | 55/63 [13:03<01:23, 10.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  89%|████████▉ | 56/63 [13:15<01:17, 11.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  90%|█████████ | 57/63 [13:36<01:24, 14.05s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  92%|█████████▏| 58/63 [13:46<01:04, 12.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  94%|█████████▎| 59/63 [13:56<00:48, 12.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  95%|█████████▌| 60/63 [14:03<00:31, 10.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  97%|█████████▋| 61/63 [14:11<00:19,  9.55s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  98%|█████████▊| 62/63 [14:22<00:10, 10.03s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating: 100%|██████████| 63/63 [14:29<00:00, 13.81s/it]\n",
            "Translating: 100%|██████████| 63/63 [38:20<00:00, 36.51s/it] \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>Менің қазір мен қасымда.</td>\n",
              "      <td>Өзiмiкiнiгiпiң кiлiшiрi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>Менің қазір, қаңтар мен қазыққа қазанды, менің...</td>\n",
              "      <td>Қандай-ақ, қанақтар мен ұзақ уақытқа дейін қар...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>Менің табылған тазалық тазыққа не айтылады.</td>\n",
              "      <td>Je n’ai pas l’intention de connaître les perso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>Менің қазір мен қаңтар менің жарыққа қадам.</td>\n",
              "      <td>Қандай-ақ, қазір мен қағидалар мен жарықтармен...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>Менің қазір мен қасым.</td>\n",
              "      <td>Je n’ai qu’à attendre.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                           Менің қазір мен қасымда.   \n",
              "1  Менің қазір, қаңтар мен қазыққа қазанды, менің...   \n",
              "2        Менің табылған тазалық тазыққа не айтылады.   \n",
              "3        Менің қазір мен қаңтар менің жарыққа қадам.   \n",
              "4                             Менің қазір мен қасым.   \n",
              "\n",
              "                                                beam  \n",
              "0                           Өзiмiкiнiгiпiң кiлiшiрi.  \n",
              "1  Қандай-ақ, қанақтар мен ұзақ уақытқа дейін қар...  \n",
              "2  Je n’ai pas l’intention de connaître les perso...  \n",
              "3  Қандай-ақ, қазір мен қағидалар мен жарықтармен...  \n",
              "4                             Je n’ai qu’à attendre.  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run using M2M\n",
        "df_m2m = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df_m2m.head())\n",
        "\n",
        "df_m2m['greedy'] = translate_select_model(df_m2m['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)\n",
        "df_m2m['beam'] = translate_select_model(df_m2m['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)\n",
        "df_m2m.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5d20159d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "from sacrebleu import corpus_chrf\n",
        "\n",
        "# Function to loop over all  dataframes defined and print out BLEU and CHRF scores\n",
        "def compute_scores(df, name=\"\"):\n",
        "    # Prepare hypothesis and reference lists\n",
        "    greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "    beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "    references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "    beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "    greedy_chrf = corpus_chrf(greedy_hypotheses, references).score\n",
        "    beam_chrf = corpus_chrf(beam_hypotheses, references).score\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{name} BLEU Scores:\")\n",
        "    print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "    print(f\"Beam BLEU:   {beam_bleu:.2f}\")\n",
        "    \n",
        "    print(f\"\\n{name} CHRF Scores:\")\n",
        "    print(f\"Greedy CHRF: {greedy_chrf:.2f}\")\n",
        "    print(f\"BEAM CHRF: {beam_chrf:.2f}\")\n",
        "    \n",
        "    return greedy_bleu, beam_bleu, greedy_chrf, beam_chrf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e3d34183",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating name: m2m\n",
            "\n",
            "m2m BLEU Scores:\n",
            "Greedy BLEU: 0.07\n",
            "Beam BLEU:   0.07\n",
            "\n",
            "m2m CHRF Scores:\n",
            "Greedy CHRF: 3.96\n",
            "BEAM CHRF: 5.22\n"
          ]
        }
      ],
      "source": [
        "results_m2m = {}\n",
        "name = \"m2m\"\n",
        "print(f\"Evaluating name: {name}\")\n",
        "greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_m2m, name)\n",
        "results_m2m[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "df_m2m.to_csv(f\"trans_res_{name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0027b2b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/62 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "# Run using M2M on OpenSubtitles dataset\n",
        "df_Open_Subtitles['greedy'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)\n",
        "df_Open_Subtitles['beam'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9e89a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translations for greedy and beam search algorithm using OpenSubtitles dataset\n",
        "df_OS_mBART = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_mBART.head())\n",
        "df_OS_mBART['greedy'] = translate(df_OS_mBART['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "df_OS_mBART['beam'] = translate(df_OS_mBART['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "df_OS_mBART.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1f3015",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Run using MarianMT on Tatoeba dataset\n",
        "# df_Tatoeba = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "# print(df_Tatoeba.head())\n",
        "\n",
        "# df_Tatoeba['greedy'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "# df_Tatoeba['beam'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)\n",
        "# df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dc650a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer_mar' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df_OS_Mar = load_parallel_corpus(\u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.en\u001b[39m\u001b[33m'\u001b[39m, max_lines=\u001b[32m1000\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_OS_Mar.head())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mgreedy\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), \u001b[43mtokenizer_mar\u001b[49m, model_mar, beam=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n\u001b[32m      6\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), tokenizer_mar, model_mar, beam=\u001b[32m5\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n",
            "\u001b[31mNameError\u001b[39m: name 'tokenizer_mar' is not defined"
          ]
        }
      ],
      "source": [
        "# Run using MarianMT on OpenSubtitles dataset\n",
        "df_OS_Mar = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_Mar.head())\n",
        "\n",
        "df_OS_Mar['greedy'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "df_OS_Mar['beam'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate algorithms using BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy BLEU: 7.94\n",
            "Beam BLEU: 9.21\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Define hypotheses for greedy and beam search (as string lists)\n",
        "greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "\n",
        "# Define references as string lists\n",
        "references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "# Calculate the bleu score\n",
        "greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "# Display bleu score\n",
        "print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "print(f\"Beam BLEU: {beam_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684d22c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a list of (name, dataframe) pairs\n",
        "dataframes = [\n",
        "    # (\"Tatoeba + mBART\", df),\n",
        "    (\"Open Subtitles_mBART\", df_OS_mBART),\n",
        "    (\"Tatoeba_m2m\", df_m2m),\n",
        "    (\"Open Subtitles_m2m\", df_Open_Subtitles),\n",
        "]\n",
        "\n",
        "# Compute BLEU and CHRF scores for each DataFrame\n",
        "results = {}\n",
        "\n",
        "# Calculate BLEU and CHRF scores for each of the data frames and save to list\n",
        "for name, df_ in dataframes:\n",
        "    try:\n",
        "        print(f\"Evaluating name: {name}\")\n",
        "        greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_, name)\n",
        "        results[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "        df_.to_csv(f\"trans_res_{name}.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Translation Comaprison Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I'm going to go to the temple.</td>\n",
              "      <td>I'm going to go to the monastery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I'm going to say I will give up a fight and go...</td>\n",
              "      <td>I said I was going to do a series I would give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I'm not a big fan of mysterious characters.</td>\n",
              "      <td>I don't think I've ever heard of a mysterious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>I'm not sure if I can do it, but I have a good...</td>\n",
              "      <td>I'm not sure if we're going to live together, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I'm only a single one.</td>\n",
              "      <td>I'm the only one who can wait.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                     I'm going to go to the temple.   \n",
              "1  I'm going to say I will give up a fight and go...   \n",
              "2        I'm not a big fan of mysterious characters.   \n",
              "3  I'm not sure if I can do it, but I have a good...   \n",
              "4                             I'm only a single one.   \n",
              "\n",
              "                                                beam  \n",
              "0                  I'm going to go to the monastery.  \n",
              "1  I said I was going to do a series I would give...  \n",
              "2  I don't think I've ever heard of a mysterious ...  \n",
              "3  I'm not sure if we're going to live together, ...  \n",
              "4                     I'm the only one who can wait.  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.to_csv(\"trans_res_Tatoeba_mBart.csv\", index=False)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459848fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_Open_Subtitles.to_csv(\"trans_res_OpenSubtitles_mBart.csv\", index=False)\n",
        "# df_Open_Subtitles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ded1e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_Tatoeba.to_csv(\"trans_res_Tatoeba_Marian.csv\", index=False)\n",
        "df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416e67cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_OS_Mar.to_csv(\"trans_res_OpenSubtitles_Marian.csv\", index=False)\n",
        "df_OS_Mar.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
