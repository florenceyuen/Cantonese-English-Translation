{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Machine Translation for Cantonese-English Language Pair\n",
        "Florence Yuen\n",
        "- Uses datasets from Tatoeba and OpenSubtitles to load Cantonese-English language pair data\n",
        "- Preprocess character-based and Jyutping romanized Cantonese data by tokenizing and doing data cleansing\n",
        "- Apply mBART-50 pre-trained multilingual NMT model\n",
        "- Compare and evaluate greedy and beam search decoding strategies\n",
        "- Save translation outputs to csv file\n",
        "- Generates BLEU scores to evaluate and compare the two decoding strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (4.54.1)\n",
            "Requirement already satisfied: datasets in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (4.0.0)\n",
            "Requirement already satisfied: sacrebleu in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: pandas in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: sentencepiece in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (0.2.0)\n",
            "Requirement already satisfied: pypinyin in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (0.55.0)\n",
            "Requirement already satisfied: epitran in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (1.26.0)\n",
            "Requirement already satisfied: hf_xet in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (1.1.5)\n",
            "Requirement already satisfied: protobuf in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (6.31.1)\n",
            "Requirement already satisfied: ipywidgets in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (8.1.7)\n",
            "Requirement already satisfied: filelock in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: portalocker in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from sacrebleu) (6.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: setuptools in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (65.5.0)\n",
            "Requirement already satisfied: panphon>=0.20 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (0.22.2)\n",
            "Requirement already satisfied: marisa-trie in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (1.2.1)\n",
            "Requirement already satisfied: jamo in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from epitran) (0.4.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (9.4.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: decorator in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: unicodecsv in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (0.14.1)\n",
            "Requirement already satisfied: editdistance in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Requirement already satisfied: munkres in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from panphon>=0.20->epitran) (1.1.4)\n",
            "Requirement already satisfied: six>=1.5 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from portalocker->sacrebleu) (311)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\florence\\homework\\universityofwaterloo\\fourthyear\\cs486\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install -r requirements.txt\n",
        "# !pip install transformers datasets sacrebleu pandas sentencepiece pypinyin epitran hf_xet protobuf ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pretrained mBART-50 NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1df670ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # for progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79483bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "# Import the tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer.src_lang = \"yue_Hant\"\n",
        "tokenizer.tgt_lang = \"en_XX\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7e979497",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3254fb92a5f47ee8f465637d2eabc0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FY\\.cache\\huggingface\\hub\\models--facebook--m2m100_418M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e5bd1271a164e0faf01a724350b97f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a17e620c1d0b4a8f859a6a2f0590c906",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5283910885744b59e1b96825c5053ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c65f9f4046340e09ed2bf3ab466b752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a3a194ef5714ae184255cd4ae7b5328",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89fe394d795943f8aed205f259553d6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9f4e7cb6a14affb0ddbab0491482df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "model_name_m2m = \"facebook/m2m100_418M\"\n",
        "tokenizer_m2m = M2M100Tokenizer.from_pretrained(model_name_m2m)\n",
        "model_m2m = M2M100ForConditionalGeneration.from_pretrained(model_name_m2m).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9402d6df",
      "metadata": {},
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Helsinki-NLP/opus-mt-yue-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/Helsinki-NLP/opus-mt-yue-en/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:476\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
            "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-688b7c5d-444991c35dfb9ec731643765;79b0df25-eba6-492a-8036-fc9b49ab2169)\n\nRepository Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-yue-en/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarianTokenizer, MarianMTModel\n\u001b[32m      4\u001b[39m model_name_mar = \u001b[33m\"\u001b[39m\u001b[33mHelsinki-NLP/opus-mt-yue-en\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tokenizer_mar = \u001b[43mMarianTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_mar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model_mar = MarianMTModel.from_pretrained(model_name_mar).to(device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1911\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   1908\u001b[39m fast_tokenizer_file = FULL_TOKENIZER_FILE\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;66;03m# Re-raise any error raised by cached_file in order to get a helpful error message\u001b[39;00m\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:318\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    261\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    262\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    263\u001b[39m     **kwargs,\n\u001b[32m    264\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    267\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:508\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    509\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    516\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    517\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    518\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    519\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mOSError\u001b[39m: Helsinki-NLP/opus-mt-yue-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "# Load MarianMT Model\n",
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "\n",
        "model_name_mar = \"Helsinki-NLP/opus-mt-yue-en\"\n",
        "tokenizer_mar = MarianTokenizer.from_pretrained(model_name_mar)\n",
        "model_mar = MarianMTModel.from_pretrained(model_name_mar).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cd541246",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "364ea28270cb45a19fd5a50885b31050",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FY\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3abb9a8a8c944f8db19a9dadbd309fac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5355474ebc2d4bcb878f4394f5e88b3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "618aabdc01274e6487237580506239ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b6674b03ce6447097e2abbcee66497a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1d15a33c3304d1b85408989c79da3d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46a71a6834dc497cbc9f91bebbad91ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e767fa8f3a6049a4812ee83c7a40db0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer_nllb = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "model_nllb = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset from Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "408b9b7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Manually load downloaded .en and .yue files, apply pre-processing to clean the text\n",
        "def load_parallel_corpus(cantonese_file, english_file, max_lines=None):\n",
        "    # Open the cantonese and english files\n",
        "    with open(cantonese_file, encoding='utf-8') as f_yue, open(english_file, encoding='utf-8') as f_en:\n",
        "        yue_lines = f_yue.readlines()\n",
        "        en_lines = f_en.readlines()\n",
        "\n",
        "    # Ensure that there is the same line count\n",
        "    if max_lines:\n",
        "        yue_lines = yue_lines[:max_lines]\n",
        "        en_lines = en_lines[:max_lines]\n",
        "\n",
        "    assert len(yue_lines) == len(en_lines), \"Line count mismatch!\"\n",
        "\n",
        "    #Apply preprocessing to clean the text \n",
        "    def clean_text(text):\n",
        "        # Remove brackets\n",
        "        text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "        text = re.sub(r'\\([^\\)]*\\)', '', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    # Clean each line of text for both cantonese and english files\n",
        "    yue_lines = [clean_text(line) for line in yue_lines]\n",
        "    en_lines = [clean_text(line) for line in en_lines]\n",
        "\n",
        "    # Build the DataFrame and drop empty lines\n",
        "    df = pd.DataFrame({'cantonese': yue_lines, 'english': en_lines})\n",
        "    df = df[(df['cantonese'] != '') & (df['english'] != '')].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def load_all_datasets(model_name):\n",
        "    dataframes={}\n",
        "    files = [\n",
        "        {\n",
        "            \"dataset_name\": \"Tatoeba\",\n",
        "            \"cantonese_file\": 'en-yue.txt/Tatoeba.en-yue.yue',\n",
        "            \"english_file\": 'en-yue.txt/Tatoeba.en-yue.en'\n",
        "        },\n",
        "        {\n",
        "            \"dataset_name\": \"Open_Subtitles\",\n",
        "            \"cantonese_file\": 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue',\n",
        "            \"english_file\": 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    max_lines=1000\n",
        "    for dataset in files:\n",
        "        df = load_parallel_corpus(cantonese_file=dataset[\"cantonese_file\"], english_file=dataset[\"english_file\"], max_lines=max_lines)\n",
        "        df_name = f\"{model_name}_{dataset['dataset_name']}\"\n",
        "        dataframes[df_name] = df\n",
        "    return dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06324449",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        }
      ],
      "source": [
        "# # Load Tatoeba dataset\n",
        "df = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5185f99b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        }
      ],
      "source": [
        "# Load 1000 lines from OpenSubtitles dataset (downloaded from Opus)\n",
        "df_Open_Subtitles = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_Open_Subtitles.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Translation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "51e51a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation function with tydm to add a loading progress bar for insights using mBART50 model\n",
        "def translate(texts, beam=1, batch_size=16):\n",
        "    translations = []\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    \n",
        "    # Devide into batches so that the progress/ percentage is shown too\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n",
        "\n",
        "# Translation function with model and tokenizer being passed in\n",
        "def translate_select_model(texts, tokenizer, model, beam=1, batch_size=16, device=\"cpu\", src_lang=None, tgt_lang=None):\n",
        "    translations = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if src_lang:\n",
        "        tokenizer.src_lang = src_lang # set source language to cantonese\n",
        "    \n",
        "    forced_bos_token_id = None\n",
        "    if tgt_lang:\n",
        "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
        "\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id = forced_bos_token_id,\n",
        "                # forced_bos_token_id=tokenizer.get_lang_id(\"en\"),  # Force target language to English\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "        if i==0:\n",
        "            print(f\"Initial translations: {batch_translations}\")\n",
        "    return translations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate & Compare Decoding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcba1fc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NLLB_7_Tatoeba':                cantonese                                            english\n",
            "0                我要去瞓覺喇。                             I have to go to sleep.\n",
            "1    我話唔定做一陣就會放棄，走去瞓晏覺算。           I may give up soon and just nap instead.\n",
            "2         我不嬲都鍾意啲神秘啲嘅人物。         I always liked mysterious characters more.\n",
            "3     雖然佢講咗對唔住，但係我都仲係好嬲。      Even though he apologized, I'm still furious.\n",
            "4                 我唯有係等。                                   I can only wait.\n",
            "..                   ...                                                ...\n",
            "995            呢件外套無衫袋㗎。                          This coat hasn't pockets.\n",
            "996     呢隻蛋糕真係冇啲人講到咁好食囉。  This cake doesn't really live up to its reputa...\n",
            "997        呢條規則唔係幾時都啱用㗎。             This rule doesn't apply to every case.\n",
            "998           呢部相機冇嗰部咁貴。       This camera is less expensive than that one.\n",
            "999   你可唔可以用呢部相機幫我哋影幅相呀？   Would you take a picture of us with this camera?\n",
            "\n",
            "[1000 rows x 2 columns], 'NLLB_7_Open_Subtitles':           cantonese                                            english\n",
            "0                你看                                              Look.\n",
            "1              笑死我了                                 - That kills me. -\n",
            "2    來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3          你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4                喂喂                                 - What? - Yoo-hoo!\n",
            "..              ...                                                ...\n",
            "977    你的共鸣有点单方面 滚开              - Your empathy is a little one-sided.\n",
            "978              好了                                           Alright!\n",
            "979        我假设你是指我吧                         I assume you mean me, too?\n",
            "980        甜心，这个很可爱                       On you it looks cute, sugar.\n",
            "981          你会办妥一切              Well, you certainly fixed everything!\n",
            "\n",
            "[982 rows x 2 columns]}\n"
          ]
        }
      ],
      "source": [
        "# Setup NLLB translation\n",
        "def translate_nllb(beam=1):\n",
        "    translate_select_model(\n",
        "        df_nllb_7['NLLB_7_Open_Subtitles']['cantonese'].tolist(), \n",
        "        tokenizer_nllb, model_nllb, \n",
        "        beam=beam, batch_size=16, \n",
        "        device=device, \n",
        "        src_lang=\"yue_Hant\", \n",
        "        tgt_lang= \"eng_Latn\"\n",
        "    )\n",
        "    \n",
        "# Run translations for greedy and beam search algorithm using Tatoeba dataset\n",
        "df_nllb_7 = load_all_datasets(\"NLLB_7\")\n",
        "\n",
        "# beam size of 7\n",
        "print(df_nllb_7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c56b304a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/62 [00:37<37:54, 37.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['You see.', 'It made me laugh.', \"Let's fix him up and show him the best shot.\", \"Aren't you going to use your brain?\", 'Oh, my God.', 'Give it to me.', 'Get out of the way.', 'A self-righteous asshole.', \"That's good.\", 'See you again.', 'Get it, you little bastards, get out of here.', \"Don't ever come back.\", \"Hey, what's going on?\", 'Hey, hold your hands, you... shoot him and keep planning the war.', 'Come on now.', 'Let me wait for you...']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 62/62 [35:10<00:00, 34.04s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>你看</td>\n",
              "      <td>Look.</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>笑死我了</td>\n",
              "      <td>- That kills me. -</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>來整蠱他吧,讓他見識最佳拍檔</td>\n",
              "      <td>Come on, we'll fix him. Let's give him the old...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>你還不會用腦嗎?</td>\n",
              "      <td>Won't you ever grow up?</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>喂喂</td>\n",
              "      <td>- What? - Yoo-hoo!</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        cantonese                                            english  beam\n",
              "0              你看                                              Look.  None\n",
              "1            笑死我了                                 - That kills me. -  None\n",
              "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...  None\n",
              "3        你還不會用腦嗎?                            Won't you ever grow up?  None\n",
              "4              喂喂                                 - What? - Yoo-hoo!  None"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nllb_7['NLLB_7_Open_Subtitles']['beam'] = translate_nllb(beam=7)\n",
        "df_nllb_7['NLLB_7_Open_Subtitles'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab63bf18",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/62 [00:42<43:17, 42.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['You see.', 'It made me laugh.', \"Let's fix him up and show him the best shot.\", \"Aren't you going to use your brain?\", 'Oh, my God.', 'Give it to me.', \"Let's get out of here.\", 'A self-proclaimed smart guy.', \"That's good.\", 'See you again.', 'Got it, you little assholes, get out of here.', \"Don't ever come back.\", \"Hey, what's going on?\", 'Hey, hold your hands, you... shoot him and keep planning the war.', 'Come on now.', 'Let me wait for you...']\n"
          ]
        }
      ],
      "source": [
        "df_nllb_10 = load_all_datasets(\"NLLB_10\")\n",
        "df_nllb_10['NLLB_10_Open_Subtitles']['beam'] = translate_nllb(beam=10)\n",
        "df_nllb_10['NLLB_10_Open_Subtitles'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2442d7b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_nllb_10['NLLB_10_Open_Subtitles']['greedy'] = translate_nllb(beam=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c2898399",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/63 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   2%|▏         | 1/63 [00:20<21:07, 20.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   3%|▎         | 2/63 [00:38<19:17, 18.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   5%|▍         | 3/63 [00:53<17:15, 17.26s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   6%|▋         | 4/63 [01:04<14:36, 14.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   8%|▊         | 5/63 [01:20<14:43, 15.23s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  10%|▉         | 6/63 [01:31<13:02, 13.73s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  11%|█         | 7/63 [01:42<11:55, 12.77s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  13%|█▎        | 8/63 [01:54<11:27, 12.50s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  14%|█▍        | 9/63 [02:04<10:43, 11.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  16%|█▌        | 10/63 [02:24<12:43, 14.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  17%|█▋        | 11/63 [02:36<11:38, 13.44s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  19%|█▉        | 12/63 [02:57<13:31, 15.90s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  21%|██        | 13/63 [03:11<12:48, 15.38s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  22%|██▏       | 14/63 [03:22<11:27, 14.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  24%|██▍       | 15/63 [03:40<12:13, 15.28s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  25%|██▌       | 16/63 [03:54<11:35, 14.79s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  27%|██▋       | 17/63 [04:08<11:08, 14.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  29%|██▊       | 18/63 [04:22<10:42, 14.29s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  30%|███       | 19/63 [04:31<09:21, 12.76s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  32%|███▏      | 20/63 [04:43<08:59, 12.55s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  33%|███▎      | 21/63 [04:59<09:32, 13.64s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  35%|███▍      | 22/63 [05:16<10:03, 14.72s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  37%|███▋      | 23/63 [05:29<09:20, 14.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  38%|███▊      | 24/63 [05:58<11:59, 18.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  40%|███▉      | 25/63 [06:30<14:21, 22.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  41%|████▏     | 26/63 [06:45<12:34, 20.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  43%|████▎     | 27/63 [07:02<11:31, 19.22s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  44%|████▍     | 28/63 [07:14<10:03, 17.25s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  46%|████▌     | 29/63 [07:24<08:30, 15.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  48%|████▊     | 30/63 [07:39<08:09, 14.85s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  49%|████▉     | 31/63 [07:49<07:10, 13.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  51%|█████     | 32/63 [08:02<06:59, 13.52s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  52%|█████▏    | 33/63 [08:11<05:57, 11.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  54%|█████▍    | 34/63 [08:24<05:55, 12.27s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  56%|█████▌    | 35/63 [08:40<06:19, 13.56s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  57%|█████▋    | 36/63 [08:50<05:35, 12.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  59%|█████▊    | 37/63 [08:59<04:54, 11.34s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  60%|██████    | 38/63 [09:13<05:04, 12.16s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  62%|██████▏   | 39/63 [09:27<05:04, 12.71s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  63%|██████▎   | 40/63 [09:44<05:19, 13.89s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  65%|██████▌   | 41/63 [09:55<04:47, 13.08s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  67%|██████▋   | 42/63 [10:06<04:26, 12.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  68%|██████▊   | 43/63 [10:42<06:30, 19.53s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  70%|██████▉   | 44/63 [11:00<06:04, 19.21s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  71%|███████▏  | 45/63 [11:13<05:08, 17.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  73%|███████▎  | 46/63 [11:23<04:14, 14.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  75%|███████▍  | 47/63 [11:38<04:01, 15.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  76%|███████▌  | 48/63 [11:47<03:17, 13.15s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  78%|███████▊  | 49/63 [11:56<02:46, 11.87s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  79%|███████▉  | 50/63 [12:02<02:11, 10.09s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  81%|████████  | 51/63 [12:16<02:18, 11.54s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  83%|████████▎ | 52/63 [12:35<02:29, 13.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  84%|████████▍ | 53/63 [12:45<02:04, 12.44s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  86%|████████▌ | 54/63 [12:56<01:49, 12.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  87%|████████▋ | 55/63 [13:03<01:23, 10.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  89%|████████▉ | 56/63 [13:15<01:17, 11.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  90%|█████████ | 57/63 [13:36<01:24, 14.05s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  92%|█████████▏| 58/63 [13:46<01:04, 12.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  94%|█████████▎| 59/63 [13:56<00:48, 12.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  95%|█████████▌| 60/63 [14:03<00:31, 10.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  97%|█████████▋| 61/63 [14:11<00:19,  9.55s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  98%|█████████▊| 62/63 [14:22<00:10, 10.03s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating: 100%|██████████| 63/63 [14:29<00:00, 13.81s/it]\n",
            "Translating: 100%|██████████| 63/63 [38:20<00:00, 36.51s/it] \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>Менің қазір мен қасымда.</td>\n",
              "      <td>Өзiмiкiнiгiпiң кiлiшiрi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>Менің қазір, қаңтар мен қазыққа қазанды, менің...</td>\n",
              "      <td>Қандай-ақ, қанақтар мен ұзақ уақытқа дейін қар...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>Менің табылған тазалық тазыққа не айтылады.</td>\n",
              "      <td>Je n’ai pas l’intention de connaître les perso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>Менің қазір мен қаңтар менің жарыққа қадам.</td>\n",
              "      <td>Қандай-ақ, қазір мен қағидалар мен жарықтармен...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>Менің қазір мен қасым.</td>\n",
              "      <td>Je n’ai qu’à attendre.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                           Менің қазір мен қасымда.   \n",
              "1  Менің қазір, қаңтар мен қазыққа қазанды, менің...   \n",
              "2        Менің табылған тазалық тазыққа не айтылады.   \n",
              "3        Менің қазір мен қаңтар менің жарыққа қадам.   \n",
              "4                             Менің қазір мен қасым.   \n",
              "\n",
              "                                                beam  \n",
              "0                           Өзiмiкiнiгiпiң кiлiшiрi.  \n",
              "1  Қандай-ақ, қанақтар мен ұзақ уақытқа дейін қар...  \n",
              "2  Je n’ai pas l’intention de connaître les perso...  \n",
              "3  Қандай-ақ, қазір мен қағидалар мен жарықтармен...  \n",
              "4                             Je n’ai qu’à attendre.  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run using M2M\n",
        "df_m2m = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df_m2m.head())\n",
        "\n",
        "df_m2m['greedy'] = translate_select_model(df_m2m['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)\n",
        "df_m2m['beam'] = translate_select_model(df_m2m['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)\n",
        "df_m2m.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "0b3168f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:11<11:49, 11.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['I have to go to bed.', 'I said, \"I\\'m going to give up and go to bed\".', 'I like mysterious characters.', 'She apologized, but I was still a little bit of a bitch.', 'I was just waiting.', 'I love you.', 'It will land on the moon tomorrow.', \"It'll land on the moon tomorrow.\", '\"Honestly, I\\'m scared of being high\". \"No more cowards!\"', '\"The phone rings\". \"I hear you\".', '\"Thank you for your help\". \"No, thank you\".', '\"What are you doing wrong?\" \"Because I don\\'t want to go to the bathroom\".', \"Democracy is the worst political system, except for the ones we've tried.\", \"Oh, that's good to see you!\", 'Can you speak Italian?', 'Do you have a condom?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 63/63 [07:14<00:00,  6.89s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I have to go to bed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I said, \"I'm going to give up and go to bed\".</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I like mysterious characters.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>She apologized, but I was still a little bit o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I was just waiting.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \n",
              "0                               I have to go to bed.  \n",
              "1      I said, \"I'm going to give up and go to bed\".  \n",
              "2                      I like mysterious characters.  \n",
              "3  She apologized, but I was still a little bit o...  \n",
              "4                                I was just waiting.  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run using M2M\n",
        "df_nllb = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df_nllb.head())\n",
        "\n",
        "df_nllb['greedy'] = translate_select_model(df_nllb['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=1, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n",
        "df_nllb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "fad2d1fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:27<28:20, 27.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['I have to go to bed.', \"I told him I'd give up and go to bed.\", 'I like more mysterious characters.', 'She apologized, but I was fine.', 'I was just waiting.', 'I love you.', 'It will land on the moon tomorrow.', 'It will land on the moon tomorrow.', '\"Honestly, I\\'m really scared\". \"Don\\'t be a coward!\"', '\"The phone rings\". \"I hear you\".', '\"Thank you very much\". \"No, thank you\".', '\"What are you not doing?\" \"Because I don\\'t want to go\".', \"Democracy is the worst political system I've ever tried.\", 'Oh, good to see you!', 'Can you speak Italian?', 'Do you have a condom?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 63/63 [25:26<00:00, 24.22s/it]\n"
          ]
        }
      ],
      "source": [
        "df_nllb['beam'] = translate_select_model(df_nllb['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=5, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang=\"eng_Latn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "2c2b4c22",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/62 [00:05<05:12,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['You see.', 'It made me laugh.', 'Get him a job and let him see the best shot.', \"You're not going to use your brain?\", 'Oh, my God.', 'Give me a round.', 'Go ahead and roll.', 'A self-righteous asshole.', \"That's good.\", 'See you again.', 'Get it, you little bastards, get out of here.', 'Never come back.', 'What are we doing?', 'Hold your hands, you... shoot him and keep planning the war.', 'Come on, you know.', 'Let me wait for you...']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 62/62 [07:05<00:00,  6.87s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>你看</td>\n",
              "      <td>Look.</td>\n",
              "      <td>You see.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>笑死我了</td>\n",
              "      <td>- That kills me. -</td>\n",
              "      <td>It made me laugh.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>來整蠱他吧,讓他見識最佳拍檔</td>\n",
              "      <td>Come on, we'll fix him. Let's give him the old...</td>\n",
              "      <td>Get him a job and let him see the best shot.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>你還不會用腦嗎?</td>\n",
              "      <td>Won't you ever grow up?</td>\n",
              "      <td>You're not going to use your brain?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>喂喂</td>\n",
              "      <td>- What? - Yoo-hoo!</td>\n",
              "      <td>Oh, my God.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        cantonese                                            english  \\\n",
              "0              你看                                              Look.   \n",
              "1            笑死我了                                 - That kills me. -   \n",
              "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...   \n",
              "3        你還不會用腦嗎?                            Won't you ever grow up?   \n",
              "4              喂喂                                 - What? - Yoo-hoo!   \n",
              "\n",
              "                                         greedy  \n",
              "0                                      You see.  \n",
              "1                             It made me laugh.  \n",
              "2  Get him a job and let him see the best shot.  \n",
              "3           You're not going to use your brain?  \n",
              "4                                   Oh, my God.  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run using M2M\n",
        "df_nllb_os = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_nllb_os.head())\n",
        "\n",
        "df_nllb_os['greedy'] = translate_select_model(df_nllb_os['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=1, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n",
        "df_nllb_os.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "aa98c926",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/62 [00:23<23:44, 23.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['You see.', 'It made me laugh.', \"Let's fix him up and show him the best shot.\", \"Aren't you going to use your brain?\", 'Oh, my God.', 'Give it to me.', 'Get out of the way.', 'A self-righteous asshole.', \"That's good.\", 'See you again.', 'Get it, you little assholes, get out of here.', \"Don't ever come back.\", \"Hey, what's going on?\", 'Okay, hold your hands, you... shoot him, and continue the war plan.', 'Come on now.', 'Let me wait for you...']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 62/62 [27:16<00:00, 26.39s/it]\n"
          ]
        }
      ],
      "source": [
        "# run beam search using nllb model on Open Subtitles dataset\n",
        "df_nllb_os['beam'] = translate_select_model(df_nllb_os['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=5, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang=\"eng_Latn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "5d20159d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "from sacrebleu import corpus_chrf\n",
        "\n",
        "# Function to loop over all  dataframes defined and print out BLEU and CHRF scores\n",
        "def compute_scores(df, name=\"\", use_greedy=True, use_beam=True):\n",
        "    greedy_bleu, greedy_chrf, beam_bleu, beam_chrf = None, None, None, None\n",
        "    references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "    if use_greedy:\n",
        "        # Prepare hypothesis and reference lists\n",
        "        greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "        # Compute BLEU scores\n",
        "        greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "        greedy_chrf = corpus_chrf(greedy_hypotheses, references).score\n",
        "\n",
        "    if use_beam:\n",
        "        beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "        beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "        beam_chrf = corpus_chrf(beam_hypotheses, references).score\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name} BLEU Scores:\")\n",
        "    if use_greedy:\n",
        "        print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "    if use_beam:\n",
        "        print(f\"Beam BLEU:   {beam_bleu:.2f}\")\n",
        "    \n",
        "    print(f\"\\n{name} CHRF Scores:\")\n",
        "    if use_greedy:\n",
        "        print(f\"Greedy CHRF: {greedy_chrf:.2f}\")\n",
        "    if use_beam:\n",
        "        print(f\"BEAM CHRF: {beam_chrf:.2f}\")\n",
        "    \n",
        "    return greedy_bleu, beam_bleu, greedy_chrf, beam_chrf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e3d34183",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating name: m2m\n",
            "\n",
            "m2m BLEU Scores:\n",
            "Greedy BLEU: 0.07\n",
            "Beam BLEU:   0.07\n",
            "\n",
            "m2m CHRF Scores:\n",
            "Greedy CHRF: 3.96\n",
            "BEAM CHRF: 5.22\n"
          ]
        }
      ],
      "source": [
        "results_m2m = {}\n",
        "name = \"m2m\"\n",
        "print(f\"Evaluating name: {name}\")\n",
        "greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_m2m, name)\n",
        "results_m2m[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "df_m2m.to_csv(f\"trans_res_{name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0027b2b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/62 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   2%|▏         | 1/62 [01:03<1:04:14, 63.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   3%|▎         | 2/62 [01:09<29:51, 29.87s/it]  The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   5%|▍         | 3/62 [01:21<21:05, 21.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   6%|▋         | 4/62 [01:28<15:25, 15.96s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   8%|▊         | 5/62 [01:39<13:13, 13.93s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  10%|▉         | 6/62 [01:50<12:04, 12.94s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  11%|█▏        | 7/62 [02:02<11:37, 12.69s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  13%|█▎        | 8/62 [02:13<11:08, 12.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  15%|█▍        | 9/62 [02:25<10:41, 12.09s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  16%|█▌        | 10/62 [02:33<09:30, 10.97s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  18%|█▊        | 11/62 [02:46<09:47, 11.52s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  19%|█▉        | 12/62 [02:54<08:45, 10.51s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  21%|██        | 13/62 [03:00<07:23,  9.06s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  23%|██▎       | 14/62 [03:09<07:19,  9.15s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  24%|██▍       | 15/62 [03:21<07:42,  9.84s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  26%|██▌       | 16/62 [03:38<09:20, 12.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  27%|██▋       | 17/62 [03:44<07:39, 10.22s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  29%|██▉       | 18/62 [03:58<08:17, 11.30s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  31%|███       | 19/62 [04:09<08:00, 11.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  32%|███▏      | 20/62 [04:16<06:57,  9.95s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  34%|███▍      | 21/62 [04:24<06:25,  9.40s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  35%|███▌      | 22/62 [04:34<06:24,  9.62s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  37%|███▋      | 23/62 [04:43<06:02,  9.30s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  39%|███▊      | 24/62 [04:58<06:58, 11.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  40%|████      | 25/62 [05:05<06:02,  9.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  42%|████▏     | 26/62 [05:12<05:24,  9.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  44%|████▎     | 27/62 [05:21<05:16,  9.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  45%|████▌     | 28/62 [05:28<04:47,  8.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  47%|████▋     | 29/62 [05:35<04:20,  7.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  48%|████▊     | 30/62 [05:44<04:29,  8.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  50%|█████     | 31/62 [05:49<03:44,  7.24s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  52%|█████▏    | 32/62 [05:59<04:03,  8.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  53%|█████▎    | 33/62 [06:07<03:54,  8.08s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  55%|█████▍    | 34/62 [06:14<03:38,  7.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  56%|█████▋    | 35/62 [06:24<03:46,  8.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  58%|█████▊    | 36/62 [06:33<03:46,  8.71s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  60%|█████▉    | 37/62 [06:40<03:19,  7.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  61%|██████▏   | 38/62 [06:44<02:48,  7.00s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  63%|██████▎   | 39/62 [06:51<02:40,  6.99s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  65%|██████▍   | 40/62 [06:58<02:34,  7.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  66%|██████▌   | 41/62 [07:07<02:37,  7.51s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  68%|██████▊   | 42/62 [07:16<02:36,  7.83s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  69%|██████▉   | 43/62 [07:21<02:12,  7.00s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  71%|███████   | 44/62 [07:28<02:07,  7.10s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  73%|███████▎  | 45/62 [07:34<01:52,  6.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  74%|███████▍  | 46/62 [07:40<01:43,  6.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  76%|███████▌  | 47/62 [07:47<01:40,  6.72s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  77%|███████▋  | 48/62 [07:54<01:37,  6.95s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  79%|███████▉  | 49/62 [08:01<01:28,  6.83s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  81%|████████  | 50/62 [08:07<01:19,  6.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  82%|████████▏ | 51/62 [08:16<01:18,  7.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  84%|████████▍ | 52/62 [08:21<01:06,  6.70s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  85%|████████▌ | 53/62 [08:30<01:04,  7.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  87%|████████▋ | 54/62 [08:41<01:07,  8.40s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  89%|████████▊ | 55/62 [08:47<00:53,  7.70s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  90%|█████████ | 56/62 [08:55<00:47,  7.97s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  92%|█████████▏| 57/62 [09:33<01:24, 16.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  94%|█████████▎| 58/62 [09:43<00:58, 14.67s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  95%|█████████▌| 59/62 [09:52<00:39, 13.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  97%|█████████▋| 60/62 [09:59<00:22, 11.16s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  98%|█████████▊| 61/62 [10:07<00:10, 10.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating: 100%|██████████| 62/62 [10:12<00:00,  9.88s/it]\n",
            "Translating:   0%|          | 0/62 [00:12<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run using M2M on OpenSubtitles dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mgreedy\u001b[39m\u001b[33m'\u001b[39m] = translate_select_model(df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), tokenizer_m2m, model_m2m, beam=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtranslate_select_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_Open_Subtitles\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcantonese\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtranslate_select_model\u001b[39m\u001b[34m(texts, tokenizer, model, beam, batch_size, device)\u001b[39m\n\u001b[32m     30\u001b[39m inputs = tokenizer(batch, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m128\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     generated_tokens = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     39\u001b[39m translations.extend(batch_translations)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2652\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2646\u001b[39m         input_ids=input_ids,\n\u001b[32m   2647\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2648\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2649\u001b[39m         **model_kwargs,\n\u001b[32m   2650\u001b[39m     )\n\u001b[32m   2651\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2652\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2662\u001b[39m     logger.warning_once(\n\u001b[32m   2663\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2664\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2665\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:4097\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4094\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   4095\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m4097\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4099\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   4100\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   4101\u001b[39m     model_outputs,\n\u001b[32m   4102\u001b[39m     model_kwargs,\n\u001b[32m   4103\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   4104\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1422\u001b[39m, in \u001b[36mM2M100ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1400\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1401\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1402\u001b[39m         )\n\u001b[32m   1404\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m   1405\u001b[39m     input_ids,\n\u001b[32m   1406\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1420\u001b[39m     cache_position=cache_position,\n\u001b[32m   1421\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m lm_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1424\u001b[39m masked_lm_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;66;03m# move labels to the correct device to enable PP\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Run using M2M on OpenSubtitles dataset\n",
        "df_Open_Subtitles['greedy'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b4e3ef57",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'yue'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtranslate_select_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_Open_Subtitles\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcantonese\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtranslate_select_model\u001b[39m\u001b[34m(texts, tokenizer, model, beam, batch_size, device)\u001b[39m\n\u001b[32m     25\u001b[39m model.to(device)\n\u001b[32m     26\u001b[39m model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msrc_lang\u001b[49m = \u001b[33m\"\u001b[39m\u001b[33myue\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# set source language to cantonese\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size), desc=\u001b[33m\"\u001b[39m\u001b[33mTranslating\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     31\u001b[39m     batch = texts[i:i+batch_size]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1075\u001b[39m, in \u001b[36mSpecialTokensMixin.__setattr__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   1073\u001b[39m     \u001b[38;5;28mself\u001b[39m._special_tokens_map[key] = value\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:194\u001b[39m, in \u001b[36mM2M100Tokenizer.src_lang\u001b[39m\u001b[34m(self, new_src_lang)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;129m@src_lang\u001b[39m.setter\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msrc_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_src_lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m._src_lang = new_src_lang\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_src_lang_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_src_lang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:348\u001b[39m, in \u001b[36mM2M100Tokenizer.set_src_lang_special_tokens\u001b[39m\u001b[34m(self, src_lang)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_src_lang_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reset the special tokens to the source lang setting. No prefix and suffix=[eos, src_lang_code].\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     lang_token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_lang_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.cur_lang_id = \u001b[38;5;28mself\u001b[39m.lang_token_to_id[lang_token]\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m.prefix_tokens = [\u001b[38;5;28mself\u001b[39m.cur_lang_id]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:361\u001b[39m, in \u001b[36mM2M100Tokenizer.get_lang_token\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_lang_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlang_code_to_token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[31mKeyError\u001b[39m: 'yue'"
          ]
        }
      ],
      "source": [
        "df_Open_Subtitles['beam'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "019283b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "m2m BLEU Scores:\n",
            "Greedy BLEU: 0.56\n",
            "\n",
            "m2m CHRF Scores:\n",
            "Greedy CHRF: 10.44\n"
          ]
        }
      ],
      "source": [
        "greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df=df_Open_Subtitles, name=name, use_beam=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9e89a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translations for greedy and beam search algorithm using OpenSubtitles dataset\n",
        "df_OS_mBART = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_mBART.head())\n",
        "df_OS_mBART['greedy'] = translate(df_OS_mBART['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "df_OS_mBART['beam'] = translate(df_OS_mBART['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "df_OS_mBART.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1f3015",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Run using MarianMT on Tatoeba dataset\n",
        "# df_Tatoeba = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "# print(df_Tatoeba.head())\n",
        "\n",
        "# df_Tatoeba['greedy'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "# df_Tatoeba['beam'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)\n",
        "# df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dc650a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer_mar' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df_OS_Mar = load_parallel_corpus(\u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.en\u001b[39m\u001b[33m'\u001b[39m, max_lines=\u001b[32m1000\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_OS_Mar.head())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mgreedy\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), \u001b[43mtokenizer_mar\u001b[49m, model_mar, beam=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n\u001b[32m      6\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), tokenizer_mar, model_mar, beam=\u001b[32m5\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n",
            "\u001b[31mNameError\u001b[39m: name 'tokenizer_mar' is not defined"
          ]
        }
      ],
      "source": [
        "# Run using MarianMT on OpenSubtitles dataset\n",
        "df_OS_Mar = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_Mar.head())\n",
        "\n",
        "df_OS_Mar['greedy'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "df_OS_Mar['beam'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate algorithms using BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy BLEU: 7.94\n",
            "Beam BLEU: 9.21\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Define hypotheses for greedy and beam search (as string lists)\n",
        "greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "\n",
        "# Define references as string lists\n",
        "references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "# Calculate the bleu score\n",
        "greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "# Display bleu score\n",
        "print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "print(f\"Beam BLEU: {beam_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "684d22c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating name: Tatoeba_NLLB, True\n",
            "Tatoeba_NLLB BLEU Scores:\n",
            "Greedy BLEU: 24.62\n",
            "Beam BLEU:   24.79\n",
            "\n",
            "Tatoeba_NLLB CHRF Scores:\n",
            "Greedy CHRF: 43.91\n",
            "BEAM CHRF: 44.18\n",
            "-------\n",
            "Evaluating name: OpenSubtitles_NLLB, True\n",
            "OpenSubtitles_NLLB BLEU Scores:\n",
            "Greedy BLEU: 14.80\n",
            "Beam BLEU:   15.54\n",
            "\n",
            "OpenSubtitles_NLLB CHRF Scores:\n",
            "Greedy CHRF: 34.10\n",
            "BEAM CHRF: 34.75\n",
            "-------\n",
            "Evaluating name: NLLB_7_Open_Subtitles, False\n",
            "NLLB_7_Open_Subtitles BLEU Scores:\n",
            "Beam BLEU:   0.00\n",
            "\n",
            "NLLB_7_Open_Subtitles CHRF Scores:\n",
            "BEAM CHRF: 2.95\n",
            "-------\n",
            "Evaluating name: Tatoeba_m2m, True\n",
            "Tatoeba_m2m BLEU Scores:\n",
            "Greedy BLEU: 0.07\n",
            "Beam BLEU:   0.07\n",
            "\n",
            "Tatoeba_m2m CHRF Scores:\n",
            "Greedy CHRF: 3.96\n",
            "BEAM CHRF: 5.22\n",
            "-------\n",
            "Evaluating name: OpenSubtitles_m2m, True\n",
            "Error evaluating OpenSubtitles_m2m: 'beam'\n"
          ]
        }
      ],
      "source": [
        "# Define a list of (name, dataframe) pairs\n",
        "use_beam = True\n",
        "use_greedy = True\n",
        "dataframes = [\n",
        "    # (\"Tatoeba + mBART\", df),\n",
        "    # (\"Open Subtitles_mBART\", df_OS_mBART),\n",
        "    (\"Tatoeba_NLLB\", df_nllb, use_greedy ),\n",
        "    (\"OpenSubtitles_NLLB\", df_nllb_os, use_greedy),\n",
        "    (\"NLLB_7_Open_Subtitles\", df_nllb_7['NLLB_7_Open_Subtitles'], False),\n",
        "    (\"Tatoeba_m2m\", df_m2m, use_greedy),\n",
        "    (\"OpenSubtitles_m2m\", df_Open_Subtitles, use_greedy),\n",
        "    # (\"NLLB_7_Tatoeba\", df_nllb_7['NLLB_7_Open_Tatoeba']),\n",
        "    \n",
        "]\n",
        "\n",
        "\n",
        "# Compute BLEU and CHRF scores for each DataFrame\n",
        "results = {}\n",
        "\n",
        "# Calculate BLEU and CHRF scores for each of the data frames and save to list\n",
        "for name, df_, use_greedy in dataframes:\n",
        "    try:\n",
        "        print(f\"Evaluating name: {name}, {use_greedy}\")\n",
        "        greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_, name, use_greedy = use_greedy)\n",
        "        results[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "        df_.to_csv(f\"trans_res_{name}.csv\", index=False)\n",
        "        print(\"-------\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Translation Comaprison Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I'm going to go to the temple.</td>\n",
              "      <td>I'm going to go to the monastery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I'm going to say I will give up a fight and go...</td>\n",
              "      <td>I said I was going to do a series I would give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I'm not a big fan of mysterious characters.</td>\n",
              "      <td>I don't think I've ever heard of a mysterious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>I'm not sure if I can do it, but I have a good...</td>\n",
              "      <td>I'm not sure if we're going to live together, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I'm only a single one.</td>\n",
              "      <td>I'm the only one who can wait.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                     I'm going to go to the temple.   \n",
              "1  I'm going to say I will give up a fight and go...   \n",
              "2        I'm not a big fan of mysterious characters.   \n",
              "3  I'm not sure if I can do it, but I have a good...   \n",
              "4                             I'm only a single one.   \n",
              "\n",
              "                                                beam  \n",
              "0                  I'm going to go to the monastery.  \n",
              "1  I said I was going to do a series I would give...  \n",
              "2  I don't think I've ever heard of a mysterious ...  \n",
              "3  I'm not sure if we're going to live together, ...  \n",
              "4                     I'm the only one who can wait.  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.to_csv(\"trans_res_Tatoeba_mBart.csv\", index=False)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459848fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_Open_Subtitles.to_csv(\"trans_res_OpenSubtitles_mBart.csv\", index=False)\n",
        "# df_Open_Subtitles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ded1e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_Tatoeba.to_csv(\"trans_res_Tatoeba_Marian.csv\", index=False)\n",
        "df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416e67cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_OS_Mar.to_csv(\"trans_res_OpenSubtitles_Marian.csv\", index=False)\n",
        "df_OS_Mar.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
