{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Machine Translation for Cantonese-English Language Pair\n",
        "Florence Yuen\n",
        "- Uses datasets from Tatoeba and OpenSubtitles to load Cantonese-English language pair data\n",
        "- Preprocess character-based and Jyutping romanized Cantonese data by tokenizing and doing data cleansing\n",
        "- Apply mBART-50 pre-trained multilingual NMT model\n",
        "- Compare and evaluate greedy and beam search decoding strategies\n",
        "- Save translation outputs to csv file\n",
        "- Generates BLEU scores to evaluate and compare the two decoding strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sacrebleu\n",
            "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: pandas in d:\\apps\\programming\\anaconda3\\lib\\site-packages (2.2.2)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
            "Collecting pypinyin\n",
            "  Using cached pypinyin-0.55.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting epitran\n",
            "  Using cached epitran-1.26.0-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting hf_xet\n",
            "  Using cached hf_xet-1.1.5-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
            "Requirement already satisfied: protobuf in d:\\apps\\programming\\anaconda3\\lib\\site-packages (4.25.3)\n",
            "Requirement already satisfied: ipywidgets in d:\\apps\\programming\\anaconda3\\lib\\site-packages (7.8.1)\n",
            "Requirement already satisfied: filelock in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from sacrebleu) (5.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: setuptools in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from epitran) (75.1.0)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Using cached panphon-0.22.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting marisa-trie (from epitran)\n",
            "  Using cached marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
            "Collecting jamo (from epitran)\n",
            "  Using cached jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.6 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipywidgets) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipywidgets) (8.27.0)\n",
            "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: decorator in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\n",
            "Requirement already satisfied: stack-data in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Using cached unicodecsv-0.14.1-py3-none-any.whl\n",
            "Collecting editdistance (from panphon>=0.20->epitran)\n",
            "  Using cached editdistance-0.8.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Using cached munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: six>=1.5 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: notebook>=4.4.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.6.6->ipywidgets) (7.2.2)\n",
            "Requirement already satisfied: pywin32>=226 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.14.1)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.27.3)\n",
            "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.5)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: tornado>=6.2.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.4.1)\n",
            "Requirement already satisfied: wcwidth in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: executing in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: asttokens in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: anyio>=3.1.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.3.0)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.4.4)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.16.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.4.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.1)\n",
            "Requirement already satisfied: pywinpty>=2.0.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.10)\n",
            "Requirement already satisfied: pyzmq>=24 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (25.1.2)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.17.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
            "Requirement already satisfied: httpx>=0.25.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.27.0)\n",
            "Requirement already satisfied: ipykernel>=6.5.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.28.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.10 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.11.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.9.6)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.0)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.7)\n",
            "Requirement already satisfied: nest-asyncio in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: psutil in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.10.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.7)\n",
            "Requirement already satisfied: rfc3339-validator in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.16.2)\n",
            "Requirement already satisfied: webencodings in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.5.1)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets)\n",
            "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets)\n",
            "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets)\n",
            "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets)\n",
            "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.5)\n",
            "Requirement already satisfied: pycparser in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.21)\n",
            "Requirement already satisfied: arrow>=0.15.0 in d:\\apps\\programming\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.3)\n",
            "Using cached transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
            "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "Using cached sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
            "Using cached pypinyin-0.55.0-py2.py3-none-any.whl (840 kB)\n",
            "Using cached epitran-1.26.0-py2.py3-none-any.whl (188 kB)\n",
            "Using cached hf_xet-1.1.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
            "Using cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
            "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Using cached panphon-0.22.2-py2.py3-none-any.whl (78 kB)\n",
            "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
            "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
            "Using cached jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Using cached marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
            "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
            "Using cached editdistance-0.8.1-cp312-cp312-win_amd64.whl (79 kB)\n",
            "Using cached munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
            "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: unicodecsv, sentencepiece, munkres, jamo, xxhash, webcolors, uri-template, safetensors, pypinyin, portalocker, multiprocess, marisa-trie, hf_xet, fqdn, editdistance, sacrebleu, huggingface-hub, tokenizers, panphon, isoduration, transformers, epitran, datasets\n",
            "Successfully installed datasets-4.0.0 editdistance-0.8.1 epitran-1.26.0 fqdn-1.5.1 hf_xet-1.1.5 huggingface-hub-0.34.3 isoduration-20.11.0 jamo-0.4.1 marisa-trie-1.2.1 multiprocess-0.70.16 munkres-1.1.4 panphon-0.22.2 portalocker-3.2.0 pypinyin-0.55.0 sacrebleu-2.5.1 safetensors-0.5.3 sentencepiece-0.2.0 tokenizers-0.21.4 transformers-4.54.1 unicodecsv-0.14.1 uri-template-1.3.0 webcolors-24.11.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# %pip install -r requirements.txt\n",
        "!pip install transformers datasets sacrebleu pandas sentencepiece pypinyin epitran hf_xet protobuf ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pretrained mBART-50 NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1df670ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # for progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cd541246",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1eb9196479d4e0eb590ac5d0e091ee8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1038b03fece24087881ef0b4ca8c455e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f28f53ec320142e4825ecaff9b6d8ac6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# from transformers import MBartForConditionalGeneration\n",
        "tokenizer_nllb = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "model_nllb = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset from Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "408b9b7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Manually load downloaded .en and .yue files, apply pre-processing to clean the text\n",
        "def load_parallel_corpus(cantonese_file, english_file, max_lines=None):\n",
        "    # Open the cantonese and english files\n",
        "    with open(cantonese_file, encoding='utf-8') as f_yue, open(english_file, encoding='utf-8') as f_en:\n",
        "        yue_lines = f_yue.readlines()\n",
        "        en_lines = f_en.readlines()\n",
        "\n",
        "    # Ensure that there is the same line count\n",
        "    if max_lines:\n",
        "        yue_lines = yue_lines[:max_lines]\n",
        "        en_lines = en_lines[:max_lines]\n",
        "\n",
        "    assert len(yue_lines) == len(en_lines), \"Line count mismatch!\"\n",
        "\n",
        "    #Apply preprocessing to clean the text \n",
        "    def clean_text(text):\n",
        "        # Remove brackets\n",
        "        text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "        text = re.sub(r'\\([^\\)]*\\)', '', text)\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    # Clean each line of text for both cantonese and english files\n",
        "    yue_lines = [clean_text(line) for line in yue_lines]\n",
        "    en_lines = [clean_text(line) for line in en_lines]\n",
        "\n",
        "    # Build the DataFrame and drop empty lines\n",
        "    df = pd.DataFrame({'cantonese': yue_lines, 'english': en_lines})\n",
        "    df = df[(df['cantonese'] != '') & (df['english'] != '')].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_all_datasets(model_name):\n",
        "    dataframes={}\n",
        "    files = [\n",
        "        {\n",
        "            \"dataset_name\": \"Tatoeba\",\n",
        "            \"cantonese_file\": 'en-yue.txt/Tatoeba.en-yue.yue',\n",
        "            \"english_file\": 'en-yue.txt/Tatoeba.en-yue.en'\n",
        "        },\n",
        "        {\n",
        "            \"dataset_name\": \"Open_Subtitles\",\n",
        "            \"cantonese_file\": 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue',\n",
        "            \"english_file\": 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    max_lines=1000\n",
        "    for dataset in files:\n",
        "        df = load_parallel_corpus(cantonese_file=dataset[\"cantonese_file\"], english_file=dataset[\"english_file\"], max_lines=max_lines)\n",
        "        df_name = f\"{model_name}_{dataset['dataset_name']}\"\n",
        "        dataframes[df_name] = df\n",
        "    return dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f6536c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes {'NLLB_Tatoeba':                cantonese                                            english\n",
            "0                我要去瞓覺喇。                             I have to go to sleep.\n",
            "1    我話唔定做一陣就會放棄，走去瞓晏覺算。           I may give up soon and just nap instead.\n",
            "2         我不嬲都鍾意啲神秘啲嘅人物。         I always liked mysterious characters more.\n",
            "3     雖然佢講咗對唔住，但係我都仲係好嬲。      Even though he apologized, I'm still furious.\n",
            "4                 我唯有係等。                                   I can only wait.\n",
            "..                   ...                                                ...\n",
            "995            呢件外套無衫袋㗎。                          This coat hasn't pockets.\n",
            "996     呢隻蛋糕真係冇啲人講到咁好食囉。  This cake doesn't really live up to its reputa...\n",
            "997        呢條規則唔係幾時都啱用㗎。             This rule doesn't apply to every case.\n",
            "998           呢部相機冇嗰部咁貴。       This camera is less expensive than that one.\n",
            "999   你可唔可以用呢部相機幫我哋影幅相呀？   Would you take a picture of us with this camera?\n",
            "\n",
            "[1000 rows x 2 columns], 'NLLB_Open_Subtitles':           cantonese                                            english\n",
            "0                你看                                              Look.\n",
            "1              笑死我了                                 - That kills me. -\n",
            "2    來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3          你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4                喂喂                                 - What? - Yoo-hoo!\n",
            "..              ...                                                ...\n",
            "977    你的共鸣有点单方面 滚开              - Your empathy is a little one-sided.\n",
            "978              好了                                           Alright!\n",
            "979        我假设你是指我吧                         I assume you mean me, too?\n",
            "980        甜心，这个很可爱                       On you it looks cute, sugar.\n",
            "981          你会办妥一切              Well, you certainly fixed everything!\n",
            "\n",
            "[982 rows x 2 columns]}\n",
            "\n",
            "DataFrames created:\n",
            "NLLB_Tatoeba: 2 rows\n",
            "NLLB_Open_Subtitles: 2 rows\n"
          ]
        }
      ],
      "source": [
        "# Load all datasets for NLLB model\n",
        "dataframes = load_all_datasets(\"NLLB\")\n",
        "print(\"Dataframes\", dataframes)\n",
        "\n",
        "print(\"\\nDataFrames created:\")\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"{name}: {len(dataframes)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06324449",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataframes = load_all_datasets(\"NLLB_10\")\n",
        "print(\"Dataframes\", dataframes)\n",
        "# beam size 10\n",
        "print(\"\\nDataFrames created:\")\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"{name}: {len(dataframes)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "5185f99b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        }
      ],
      "source": [
        "# Load 1000 lines from OpenSubtitles dataset (downloaded from Opus)\n",
        "df_Open_Subtitles = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_Open_Subtitles.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Translation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "51e51a9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation function with tydm to add a loading progress bar for insights using mBART50 model\n",
        "def translate(texts, beam=1, batch_size=16):\n",
        "    translations = []\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    \n",
        "    # Devide into batches so that the progress/ percentage is shown too\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "    return translations\n",
        "\n",
        "# Translation function with model and tokenizer being passed in\n",
        "def translate_select_model(texts, tokenizer, model, beam=1, batch_size=16, device=\"cpu\", src_lang=None, tgt_lang=None):\n",
        "    translations = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if src_lang:\n",
        "        tokenizer.src_lang = src_lang # set source language to cantonese\n",
        "    \n",
        "    forced_bos_token_id = None\n",
        "    if tgt_lang:\n",
        "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
        "\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                forced_bos_token_id = forced_bos_token_id,\n",
        "                # forced_bos_token_id=tokenizer.get_lang_id(\"en\"),  # Force target language to English\n",
        "                max_length=128,\n",
        "                num_beams=beam,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translations.extend(batch_translations)\n",
        "        if i==0:\n",
        "            print(f\"Initial translations: {batch_translations}\")\n",
        "    return translations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate & Compare Decoding Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "dcba1fc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'NLLB_Tatoeba':                cantonese                                            english\n",
            "0                我要去瞓覺喇。                             I have to go to sleep.\n",
            "1    我話唔定做一陣就會放棄，走去瞓晏覺算。           I may give up soon and just nap instead.\n",
            "2         我不嬲都鍾意啲神秘啲嘅人物。         I always liked mysterious characters more.\n",
            "3     雖然佢講咗對唔住，但係我都仲係好嬲。      Even though he apologized, I'm still furious.\n",
            "4                 我唯有係等。                                   I can only wait.\n",
            "..                   ...                                                ...\n",
            "995            呢件外套無衫袋㗎。                          This coat hasn't pockets.\n",
            "996     呢隻蛋糕真係冇啲人講到咁好食囉。  This cake doesn't really live up to its reputa...\n",
            "997        呢條規則唔係幾時都啱用㗎。             This rule doesn't apply to every case.\n",
            "998           呢部相機冇嗰部咁貴。       This camera is less expensive than that one.\n",
            "999   你可唔可以用呢部相機幫我哋影幅相呀？   Would you take a picture of us with this camera?\n",
            "\n",
            "[1000 rows x 2 columns], 'NLLB_Open_Subtitles':           cantonese                                            english\n",
            "0                你看                                              Look.\n",
            "1              笑死我了                                 - That kills me. -\n",
            "2    來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3          你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4                喂喂                                 - What? - Yoo-hoo!\n",
            "..              ...                                                ...\n",
            "977    你的共鸣有点单方面 滚开              - Your empathy is a little one-sided.\n",
            "978              好了                                           Alright!\n",
            "979        我假设你是指我吧                         I assume you mean me, too?\n",
            "980        甜心，这个很可爱                       On you it looks cute, sugar.\n",
            "981          你会办妥一切              Well, you certainly fixed everything!\n",
            "\n",
            "[982 rows x 2 columns]}\n"
          ]
        }
      ],
      "source": [
        "# Run translations for greedy and beam search algorithm using Tatoeba dataset\n",
        "print(dataframes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "f881535d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:16<17:32, 16.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['I have to go to bed.', \"I told him I'd give up and go to bed.\", 'I like more mysterious characters.', \"He apologized, but I'm still cool.\", 'I was just waiting.', 'I love you.', 'It will land on the moon tomorrow.', 'It will land on the moon tomorrow.', '\"Honestly, I\\'m really scared\". \"Don\\'t be a coward!\"', '\"The phone rings\". \"I hear you\".', '\"Thank you very much\". \"No, thank you\".', '\"What are you doing?\" \"Because I don\\'t want to go\".', \"Democracy is the worst political system I've ever tried.\", \"Oh, that's good to see you!\", 'Can you speak Italian?', 'Do you have a condom?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 63/63 [23:23<00:00, 22.27s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I have to go to bed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I told him I'd give up and go to bed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I like more mysterious characters.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>He apologized, but I'm still cool.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I was just waiting.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                    beam  \n",
              "0                   I have to go to bed.  \n",
              "1  I told him I'd give up and go to bed.  \n",
              "2     I like more mysterious characters.  \n",
              "3     He apologized, but I'm still cool.  \n",
              "4                    I was just waiting.  "
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframes['NLLB_Tatoeba']['beam'] = translate_select_model(dataframes['NLLB_Tatoeba']['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=3, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n",
        "dataframes['NLLB_Tatoeba'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "1d19f97a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:18<18:50, 18.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['I have to go to bed.', 'I said, \"I\\'m going to give up and go to bed\".', 'I like mysterious characters.', 'She apologized, but I was still a little bit of a bitch.', 'I was just waiting.', 'I love you.', 'It will land on the moon tomorrow.', \"It'll land on the moon tomorrow.\", '\"Honestly, I\\'m scared of being high\". \"No more cowards!\"', '\"The phone rings\". \"I hear you\".', '\"Thank you for your help\". \"No, thank you\".', '\"What are you doing wrong?\" \"Because I don\\'t want to go to the bathroom\".', \"Democracy is the worst political system, except for the ones we've tried.\", \"Oh, that's good to see you!\", 'Can you speak Italian?', 'Do you have a condom?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 63/63 [13:36<00:00, 12.96s/it]\n"
          ]
        }
      ],
      "source": [
        "dataframes['NLLB_Tatoeba']['greedy'] = translate_select_model(dataframes['NLLB_Tatoeba']['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=1, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "e6ea9992",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/62 [00:16<16:36, 16.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['You see.', 'It made me laugh.', \"Let's fix him up and show him the best shot.\", \"You're not going to use your brain?\", 'Oh, my God.', 'Give it to me.', 'Get out of the way.', 'A self-righteous asshole.', \"That's good.\", 'See you again.', 'I got you, you little bastards, get out of here.', \"Don't ever come back.\", \"Hey, what's going on?\", 'Okay, hold your hands, you... shoot him, and continue the war plan.', 'Come on now.', 'Let me wait for you...']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 62/62 [29:45<00:00, 28.80s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>你看</td>\n",
              "      <td>Look.</td>\n",
              "      <td>You see.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>笑死我了</td>\n",
              "      <td>- That kills me. -</td>\n",
              "      <td>It made me laugh.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>來整蠱他吧,讓他見識最佳拍檔</td>\n",
              "      <td>Come on, we'll fix him. Let's give him the old...</td>\n",
              "      <td>Let's fix him up and show him the best shot.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>你還不會用腦嗎?</td>\n",
              "      <td>Won't you ever grow up?</td>\n",
              "      <td>You're not going to use your brain?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>喂喂</td>\n",
              "      <td>- What? - Yoo-hoo!</td>\n",
              "      <td>Oh, my God.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        cantonese                                            english  \\\n",
              "0              你看                                              Look.   \n",
              "1            笑死我了                                 - That kills me. -   \n",
              "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...   \n",
              "3        你還不會用腦嗎?                            Won't you ever grow up?   \n",
              "4              喂喂                                 - What? - Yoo-hoo!   \n",
              "\n",
              "                                           beam  \n",
              "0                                      You see.  \n",
              "1                             It made me laugh.  \n",
              "2  Let's fix him up and show him the best shot.  \n",
              "3           You're not going to use your brain?  \n",
              "4                                   Oh, my God.  "
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframes['NLLB_Open_Subtitles']['beam'] = translate_select_model(dataframes['NLLB_Open_Subtitles']['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=3, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n",
        "dataframes['NLLB_Open_Subtitles'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "c8da243c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/62 [00:14<14:37, 14.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['You see.', 'It made me laugh.', 'Get him a job and let him see the best shot.', \"You're not going to use your brain?\", 'Oh, my God.', 'Give me a round.', 'Go ahead and roll.', 'A self-righteous asshole.', \"That's good.\", 'See you again.', 'Get it, you little bastards, get out of here.', 'Never come back.', 'What are we doing?', 'Hold your hands, you... shoot him and keep planning the war.', 'Come on, you know.', 'Let me wait for you...']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating: 100%|██████████| 62/62 [18:52<00:00, 18.27s/it]\n"
          ]
        }
      ],
      "source": [
        "dataframes['NLLB_Open_Subtitles']['greedy'] = translate_select_model(dataframes['NLLB_Open_Subtitles']['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=1, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3168f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             cantonese                                        english\n",
            "0              我要去瞓覺喇。                         I have to go to sleep.\n",
            "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.\n",
            "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.\n",
            "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.\n",
            "4               我唯有係等。                               I can only wait.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/63 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:17<17:34, 17.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['I have to go to bed.', 'I said, \"I\\'m going to give up and go to bed\".', 'I like mysterious characters.', 'She apologized, but I was still a little bit of a bitch.', 'I was just waiting.', 'I love you.', 'It will land on the moon tomorrow.', \"It'll land on the moon tomorrow.\", '\"Honestly, I\\'m scared of being high\". \"No more cowards!\"', '\"The phone rings\". \"I hear you\".', '\"Thank you for your help\". \"No, thank you\".', '\"What are you doing wrong?\" \"Because I don\\'t want to go to the bathroom\".', \"Democracy is the worst political system, except for the ones we've tried.\", \"Oh, that's good to see you!\", 'Can you speak Italian?', 'Do you have a condom?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:27<28:33, 27.64s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df_nllb \u001b[38;5;241m=\u001b[39m load_parallel_corpus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-yue.txt/Tatoeba.en-yue.yue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-yue.txt/Tatoeba.en-yue.en\u001b[39m\u001b[38;5;124m'\u001b[39m, max_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_nllb\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m----> 5\u001b[0m df_nllb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m translate_select_model(df_nllb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcantonese\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), tokenizer_nllb, model_nllb, beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, src_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myue_Hant\u001b[39m\u001b[38;5;124m\"\u001b[39m, tgt_lang\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng_Latn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m df_nllb\u001b[38;5;241m.\u001b[39mhead()\n",
            "Cell \u001b[1;32mIn[41], line 40\u001b[0m, in \u001b[0;36mtranslate_select_model\u001b[1;34m(texts, tokenizer, model, beam, batch_size, device, src_lang, tgt_lang)\u001b[0m\n\u001b[0;32m     38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 40\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     42\u001b[0m         forced_bos_token_id \u001b[38;5;241m=\u001b[39m forced_bos_token_id,\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# forced_bos_token_id=tokenizer.get_lang_id(\"en\"),  # Force target language to English\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     45\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mbeam,\n\u001b[0;32m     46\u001b[0m         no_repeat_ngram_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     48\u001b[0m batch_translations \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m translations\u001b[38;5;241m.\u001b[39mextend(batch_translations)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2633\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2625\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2626\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2627\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2628\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2629\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2630\u001b[0m     )\n\u001b[0;32m   2632\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2633\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2634\u001b[0m         input_ids,\n\u001b[0;32m   2635\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2636\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2637\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2638\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2639\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2640\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2641\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2644\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2645\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2646\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2647\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2648\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2649\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2650\u001b[0m     )\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:3617\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3615\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3617\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3619\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3620\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3621\u001b[0m     outputs,\n\u001b[0;32m   3622\u001b[0m     model_kwargs,\n\u001b[0;32m   3623\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3624\u001b[0m )\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1404\u001b[0m, in \u001b[0;36mM2M100ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1400\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1401\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1402\u001b[0m         )\n\u001b[1;32m-> 1404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1405\u001b[0m     input_ids,\n\u001b[0;32m   1406\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1407\u001b[0m     decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1408\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39mencoder_outputs,\n\u001b[0;32m   1409\u001b[0m     decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1410\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1411\u001b[0m     decoder_head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1412\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1413\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1414\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1415\u001b[0m     decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1416\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1417\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1418\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1419\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1420\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1421\u001b[0m )\n\u001b[0;32m   1422\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1424\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1279\u001b[0m, in \u001b[0;36mM2M100Model.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1273\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1274\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1275\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1279\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m   1280\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1281\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1282\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1283\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1284\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1285\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1286\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1287\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1288\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1289\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1290\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1291\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1292\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1293\u001b[0m )\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1127\u001b[0m, in \u001b[0;36mM2M100Decoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1123\u001b[0m skip_the_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m (dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_the_layer \u001b[38;5;129;01mor\u001b[39;00m synced_gpus:\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;66;03m# under fsdp or deepspeed zero3 all gpus must run in sync\u001b[39;00m\n\u001b[1;32m-> 1127\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m   1128\u001b[0m         hidden_states,\n\u001b[0;32m   1129\u001b[0m         attention_mask,\n\u001b[0;32m   1130\u001b[0m         encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1132\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1133\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             cross_attn_head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m cross_attn_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         ),\n\u001b[0;32m   1136\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1137\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1138\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1139\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1140\u001b[0m     )\n\u001b[0;32m   1142\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:473\u001b[0m, in \u001b[0;36mM2M100DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    470\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    474\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    475\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    476\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    477\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    478\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    479\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    480\u001b[0m )\n\u001b[0;32m    481\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    482\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:281\u001b[0m, in \u001b[0;36mM2M100Attention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m kv_input_shape \u001b[38;5;241m=\u001b[39m (bsz, src_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mq_input_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_value, EncoderDecoderCache):\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\apps\\Programming\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run using M2M\n",
        "df_nllb = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "print(df_nllb.head())\n",
        "\n",
        "df_nllb['greedy'] = translate_select_model(df_nllb['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=1, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang= \"eng_Latn\")\n",
        "df_nllb.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad2d1fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   2%|▏         | 1/63 [00:27<28:20, 27.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial translations: ['I have to go to bed.', \"I told him I'd give up and go to bed.\", 'I like more mysterious characters.', 'She apologized, but I was fine.', 'I was just waiting.', 'I love you.', 'It will land on the moon tomorrow.', 'It will land on the moon tomorrow.', '\"Honestly, I\\'m really scared\". \"Don\\'t be a coward!\"', '\"The phone rings\". \"I hear you\".', '\"Thank you very much\". \"No, thank you\".', '\"What are you not doing?\" \"Because I don\\'t want to go\".', \"Democracy is the worst political system I've ever tried.\", 'Oh, good to see you!', 'Can you speak Italian?', 'Do you have a condom?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:  89%|████████▉ | 56/63 [22:34<02:57, 25.29s/it]"
          ]
        }
      ],
      "source": [
        "df_nllb['beam'] = translate_select_model(df_nllb['cantonese'].tolist(), tokenizer_nllb, model_nllb, beam=5, batch_size=16, device=device, src_lang=\"yue_Hant\", tgt_lang=\"eng_Latn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d20159d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "from sacrebleu import corpus_chrf\n",
        "\n",
        "# Function to loop over all  dataframes defined and print out BLEU and CHRF scores\n",
        "def compute_scores(df, name=\"\", use_beam=True):\n",
        "    greedy_bleu, greedy_chrf, beam_bleu, beam_chrf = None, None, None, None\n",
        "    references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "    # Prepare hypothesis and reference lists\n",
        "    greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "    # Compute BLEU scores\n",
        "    greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "    greedy_chrf = corpus_chrf(greedy_hypotheses, references).score\n",
        "\n",
        "    if use_beam:\n",
        "        beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "        beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "        beam_chrf = corpus_chrf(beam_hypotheses, references).score\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name} BLEU Scores:\")\n",
        "    print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "    if use_beam:\n",
        "        print(f\"Beam BLEU:   {beam_bleu:.2f}\")\n",
        "    \n",
        "    print(f\"\\n{name} CHRF Scores:\")\n",
        "    print(f\"Greedy CHRF: {greedy_chrf:.2f}\")\n",
        "    if use_beam:\n",
        "        print(f\"BEAM CHRF: {beam_chrf:.2f}\")\n",
        "    \n",
        "    return greedy_bleu, beam_bleu, greedy_chrf, beam_chrf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d34183",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating name: m2m\n",
            "\n",
            "m2m BLEU Scores:\n",
            "Greedy BLEU: 0.07\n",
            "Beam BLEU:   0.07\n",
            "\n",
            "m2m CHRF Scores:\n",
            "Greedy CHRF: 3.96\n",
            "BEAM CHRF: 5.22\n"
          ]
        }
      ],
      "source": [
        "results_m2m = {}\n",
        "name = \"m2m\"\n",
        "print(f\"Evaluating name: {name}\")\n",
        "greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df_m2m, name)\n",
        "results_m2m[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "df_m2m.to_csv(f\"trans_res_{name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0027b2b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating:   0%|          | 0/62 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   2%|▏         | 1/62 [01:03<1:04:14, 63.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   3%|▎         | 2/62 [01:09<29:51, 29.87s/it]  The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   5%|▍         | 3/62 [01:21<21:05, 21.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   6%|▋         | 4/62 [01:28<15:25, 15.96s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:   8%|▊         | 5/62 [01:39<13:13, 13.93s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  10%|▉         | 6/62 [01:50<12:04, 12.94s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  11%|█▏        | 7/62 [02:02<11:37, 12.69s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  13%|█▎        | 8/62 [02:13<11:08, 12.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  15%|█▍        | 9/62 [02:25<10:41, 12.09s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  16%|█▌        | 10/62 [02:33<09:30, 10.97s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  18%|█▊        | 11/62 [02:46<09:47, 11.52s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  19%|█▉        | 12/62 [02:54<08:45, 10.51s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  21%|██        | 13/62 [03:00<07:23,  9.06s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  23%|██▎       | 14/62 [03:09<07:19,  9.15s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  24%|██▍       | 15/62 [03:21<07:42,  9.84s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  26%|██▌       | 16/62 [03:38<09:20, 12.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  27%|██▋       | 17/62 [03:44<07:39, 10.22s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  29%|██▉       | 18/62 [03:58<08:17, 11.30s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  31%|███       | 19/62 [04:09<08:00, 11.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  32%|███▏      | 20/62 [04:16<06:57,  9.95s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  34%|███▍      | 21/62 [04:24<06:25,  9.40s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  35%|███▌      | 22/62 [04:34<06:24,  9.62s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  37%|███▋      | 23/62 [04:43<06:02,  9.30s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  39%|███▊      | 24/62 [04:58<06:58, 11.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  40%|████      | 25/62 [05:05<06:02,  9.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  42%|████▏     | 26/62 [05:12<05:24,  9.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  44%|████▎     | 27/62 [05:21<05:16,  9.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  45%|████▌     | 28/62 [05:28<04:47,  8.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  47%|████▋     | 29/62 [05:35<04:20,  7.91s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  48%|████▊     | 30/62 [05:44<04:29,  8.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  50%|█████     | 31/62 [05:49<03:44,  7.24s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  52%|█████▏    | 32/62 [05:59<04:03,  8.12s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  53%|█████▎    | 33/62 [06:07<03:54,  8.08s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  55%|█████▍    | 34/62 [06:14<03:38,  7.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  56%|█████▋    | 35/62 [06:24<03:46,  8.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  58%|█████▊    | 36/62 [06:33<03:46,  8.71s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  60%|█████▉    | 37/62 [06:40<03:19,  7.98s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  61%|██████▏   | 38/62 [06:44<02:48,  7.00s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  63%|██████▎   | 39/62 [06:51<02:40,  6.99s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  65%|██████▍   | 40/62 [06:58<02:34,  7.04s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  66%|██████▌   | 41/62 [07:07<02:37,  7.51s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  68%|██████▊   | 42/62 [07:16<02:36,  7.83s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  69%|██████▉   | 43/62 [07:21<02:12,  7.00s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  71%|███████   | 44/62 [07:28<02:07,  7.10s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  73%|███████▎  | 45/62 [07:34<01:52,  6.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  74%|███████▍  | 46/62 [07:40<01:43,  6.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  76%|███████▌  | 47/62 [07:47<01:40,  6.72s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  77%|███████▋  | 48/62 [07:54<01:37,  6.95s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  79%|███████▉  | 49/62 [08:01<01:28,  6.83s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  81%|████████  | 50/62 [08:07<01:19,  6.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  82%|████████▏ | 51/62 [08:16<01:18,  7.17s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  84%|████████▍ | 52/62 [08:21<01:06,  6.70s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  85%|████████▌ | 53/62 [08:30<01:04,  7.18s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  87%|████████▋ | 54/62 [08:41<01:07,  8.40s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  89%|████████▊ | 55/62 [08:47<00:53,  7.70s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  90%|█████████ | 56/62 [08:55<00:47,  7.97s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  92%|█████████▏| 57/62 [09:33<01:24, 16.81s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  94%|█████████▎| 58/62 [09:43<00:58, 14.67s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  95%|█████████▌| 59/62 [09:52<00:39, 13.01s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  97%|█████████▋| 60/62 [09:59<00:22, 11.16s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating:  98%|█████████▊| 61/62 [10:07<00:10, 10.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Translating: 100%|██████████| 62/62 [10:12<00:00,  9.88s/it]\n",
            "Translating:   0%|          | 0/62 [00:12<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run using M2M on OpenSubtitles dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mgreedy\u001b[39m\u001b[33m'\u001b[39m] = translate_select_model(df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), tokenizer_m2m, model_m2m, beam=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtranslate_select_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_Open_Subtitles\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcantonese\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtranslate_select_model\u001b[39m\u001b[34m(texts, tokenizer, model, beam, batch_size, device)\u001b[39m\n\u001b[32m     30\u001b[39m inputs = tokenizer(batch, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m128\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     generated_tokens = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m batch_translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     39\u001b[39m translations.extend(batch_translations)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2652\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2646\u001b[39m         input_ids=input_ids,\n\u001b[32m   2647\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2648\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2649\u001b[39m         **model_kwargs,\n\u001b[32m   2650\u001b[39m     )\n\u001b[32m   2651\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2652\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2662\u001b[39m     logger.warning_once(\n\u001b[32m   2663\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2664\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2665\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:4097\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4094\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   4095\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m4097\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4099\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   4100\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   4101\u001b[39m     model_outputs,\n\u001b[32m   4102\u001b[39m     model_kwargs,\n\u001b[32m   4103\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   4104\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1422\u001b[39m, in \u001b[36mM2M100ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1400\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1401\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1402\u001b[39m         )\n\u001b[32m   1404\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m   1405\u001b[39m     input_ids,\n\u001b[32m   1406\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1420\u001b[39m     cache_position=cache_position,\n\u001b[32m   1421\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m lm_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1424\u001b[39m masked_lm_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;66;03m# move labels to the correct device to enable PP\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Run using M2M on OpenSubtitles dataset\n",
        "df_Open_Subtitles['greedy'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=1, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e3ef57",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'yue'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_Open_Subtitles[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtranslate_select_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_Open_Subtitles\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcantonese\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_m2m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtranslate_select_model\u001b[39m\u001b[34m(texts, tokenizer, model, beam, batch_size, device)\u001b[39m\n\u001b[32m     25\u001b[39m model.to(device)\n\u001b[32m     26\u001b[39m model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msrc_lang\u001b[49m = \u001b[33m\"\u001b[39m\u001b[33myue\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# set source language to cantonese\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size), desc=\u001b[33m\"\u001b[39m\u001b[33mTranslating\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     31\u001b[39m     batch = texts[i:i+batch_size]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1075\u001b[39m, in \u001b[36mSpecialTokensMixin.__setattr__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   1073\u001b[39m     \u001b[38;5;28mself\u001b[39m._special_tokens_map[key] = value\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:194\u001b[39m, in \u001b[36mM2M100Tokenizer.src_lang\u001b[39m\u001b[34m(self, new_src_lang)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;129m@src_lang\u001b[39m.setter\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msrc_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_src_lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m._src_lang = new_src_lang\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_src_lang_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_src_lang\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:348\u001b[39m, in \u001b[36mM2M100Tokenizer.set_src_lang_special_tokens\u001b[39m\u001b[34m(self, src_lang)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_src_lang_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reset the special tokens to the source lang setting. No prefix and suffix=[eos, src_lang_code].\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     lang_token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_lang_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.cur_lang_id = \u001b[38;5;28mself\u001b[39m.lang_token_to_id[lang_token]\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m.prefix_tokens = [\u001b[38;5;28mself\u001b[39m.cur_lang_id]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\Florence\\Homework\\UniversityOfWaterloo\\FourthYear\\CS486\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\tokenization_m2m_100.py:361\u001b[39m, in \u001b[36mM2M100Tokenizer.get_lang_token\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_lang_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlang_code_to_token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[31mKeyError\u001b[39m: 'yue'"
          ]
        }
      ],
      "source": [
        "df_Open_Subtitles['beam'] = translate_select_model(df_Open_Subtitles['cantonese'].tolist(), tokenizer_m2m, model_m2m, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019283b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "m2m BLEU Scores:\n",
            "Greedy BLEU: 0.56\n",
            "\n",
            "m2m CHRF Scores:\n",
            "Greedy CHRF: 10.44\n"
          ]
        }
      ],
      "source": [
        "greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df=df_Open_Subtitles, name=name, use_beam=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9e89a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translations for greedy and beam search algorithm using OpenSubtitles dataset\n",
        "df_OS_mBART = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_mBART.head())\n",
        "df_OS_mBART['greedy'] = translate(df_OS_mBART['cantonese'].tolist(), beam=1, batch_size=16)\n",
        "df_OS_mBART['beam'] = translate(df_OS_mBART['cantonese'].tolist(), beam=5, batch_size=16)\n",
        "df_OS_mBART.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1f3015",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Run using MarianMT on Tatoeba dataset\n",
        "# df_Tatoeba = load_parallel_corpus('en-yue.txt/Tatoeba.en-yue.yue', 'en-yue.txt/Tatoeba.en-yue.en', max_lines=1000)\n",
        "# print(df_Tatoeba.head())\n",
        "\n",
        "# df_Tatoeba['greedy'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "# df_Tatoeba['beam'] = translate(df_Tatoeba['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)\n",
        "# df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dc650a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        cantonese                                            english\n",
            "0              你看                                              Look.\n",
            "1            笑死我了                                 - That kills me. -\n",
            "2  來整蠱他吧,讓他見識最佳拍檔  Come on, we'll fix him. Let's give him the old...\n",
            "3        你還不會用腦嗎?                            Won't you ever grow up?\n",
            "4              喂喂                                 - What? - Yoo-hoo!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer_mar' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df_OS_Mar = load_parallel_corpus(\u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mopen-subtitles_en-yue.txt/OpenSubtitles.en-yue.en\u001b[39m\u001b[33m'\u001b[39m, max_lines=\u001b[32m1000\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_OS_Mar.head())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mgreedy\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), \u001b[43mtokenizer_mar\u001b[49m, model_mar, beam=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n\u001b[32m      6\u001b[39m df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mbeam\u001b[39m\u001b[33m'\u001b[39m] = translate(df_OS_Mar[\u001b[33m'\u001b[39m\u001b[33mcantonese\u001b[39m\u001b[33m'\u001b[39m].tolist(), tokenizer_mar, model_mar, beam=\u001b[32m5\u001b[39m, batch_size=\u001b[32m16\u001b[39m, device=device)\n",
            "\u001b[31mNameError\u001b[39m: name 'tokenizer_mar' is not defined"
          ]
        }
      ],
      "source": [
        "# Run using MarianMT on OpenSubtitles dataset\n",
        "df_OS_Mar = load_parallel_corpus('open-subtitles_en-yue.txt/OpenSubtitles.en-yue.yue', 'open-subtitles_en-yue.txt/OpenSubtitles.en-yue.en', max_lines=1000)\n",
        "print(df_OS_Mar.head())\n",
        "\n",
        "df_OS_Mar['greedy'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=1, batch_size=16, device=device)\n",
        "df_OS_Mar['beam'] = translate(df_OS_Mar['cantonese'].tolist(), tokenizer_mar, model_mar, beam=5, batch_size=16, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate algorithms using BLEU scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy BLEU: 7.94\n",
            "Beam BLEU: 9.21\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Define hypotheses for greedy and beam search (as string lists)\n",
        "greedy_hypotheses = df['greedy'].astype(str).tolist()\n",
        "beam_hypotheses = df['beam'].astype(str).tolist()\n",
        "\n",
        "# Define references as string lists\n",
        "references = [df['english'].astype(str).tolist()]\n",
        "\n",
        "# Calculate the bleu score\n",
        "greedy_bleu = corpus_bleu(greedy_hypotheses, references).score\n",
        "beam_bleu = corpus_bleu(beam_hypotheses, references).score\n",
        "\n",
        "# Display bleu score\n",
        "print(f\"Greedy BLEU: {greedy_bleu:.2f}\")\n",
        "print(f\"Beam BLEU: {beam_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "684d22c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating name: NLLB_Tatoeba\n",
            "\n",
            "NLLB_Tatoeba BLEU Scores:\n",
            "Greedy BLEU: 24.62\n",
            "Beam BLEU:   24.70\n",
            "\n",
            "NLLB_Tatoeba CHRF Scores:\n",
            "Greedy CHRF: 43.91\n",
            "BEAM CHRF: 44.17\n",
            "\n",
            "Evaluating name: NLLB_Open_Subtitles\n",
            "\n",
            "NLLB_Open_Subtitles BLEU Scores:\n",
            "Greedy BLEU: 14.80\n",
            "Beam BLEU:   15.43\n",
            "\n",
            "NLLB_Open_Subtitles CHRF Scores:\n",
            "Greedy CHRF: 34.10\n",
            "BEAM CHRF: 34.50\n"
          ]
        }
      ],
      "source": [
        "# Define a list of (name, dataframe) pairs\n",
        "# dataframes = [\n",
        "#     # (\"Tatoeba + mBART\", df),\n",
        "#     (\"Open Subtitles_mBART\", df_OS_mBART),\n",
        "#     (\"Tatoeba_m2m\", df_m2m),\n",
        "#     (\"Open Subtitles_m2m\", df_Open_Subtitles),\n",
        "# ]\n",
        "\n",
        "# Compute BLEU and CHRF scores for each DataFrame\n",
        "results = {}\n",
        "\n",
        "# Calculate BLEU and CHRF scores for each of the data frames and save to list\n",
        "# for name, df_ in dataframes:\n",
        "for name, df in dataframes.items():\n",
        "    try:\n",
        "        print(f\"\\nEvaluating name: {name}\")\n",
        "        greedy_bleu, beam_bleu, greedy_chrf, beam_chrf = compute_scores(df, name)\n",
        "        results[name] = {\"greedy_bleu\": greedy_bleu, \"beam_bleu\": beam_bleu, \"greedy_chrf\": greedy_chrf, \"beam_chrf\": beam_chrf}\n",
        "        df.to_csv(f\"trans_res_{name}.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Translation Comaprison Results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cantonese</th>\n",
              "      <th>english</th>\n",
              "      <th>greedy</th>\n",
              "      <th>beam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>我要去瞓覺喇。</td>\n",
              "      <td>I have to go to sleep.</td>\n",
              "      <td>I'm going to go to the temple.</td>\n",
              "      <td>I'm going to go to the monastery.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>我話唔定做一陣就會放棄，走去瞓晏覺算。</td>\n",
              "      <td>I may give up soon and just nap instead.</td>\n",
              "      <td>I'm going to say I will give up a fight and go...</td>\n",
              "      <td>I said I was going to do a series I would give...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我不嬲都鍾意啲神秘啲嘅人物。</td>\n",
              "      <td>I always liked mysterious characters more.</td>\n",
              "      <td>I'm not a big fan of mysterious characters.</td>\n",
              "      <td>I don't think I've ever heard of a mysterious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>雖然佢講咗對唔住，但係我都仲係好嬲。</td>\n",
              "      <td>Even though he apologized, I'm still furious.</td>\n",
              "      <td>I'm not sure if I can do it, but I have a good...</td>\n",
              "      <td>I'm not sure if we're going to live together, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我唯有係等。</td>\n",
              "      <td>I can only wait.</td>\n",
              "      <td>I'm only a single one.</td>\n",
              "      <td>I'm the only one who can wait.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             cantonese                                        english  \\\n",
              "0              我要去瞓覺喇。                         I have to go to sleep.   \n",
              "1  我話唔定做一陣就會放棄，走去瞓晏覺算。       I may give up soon and just nap instead.   \n",
              "2       我不嬲都鍾意啲神秘啲嘅人物。     I always liked mysterious characters more.   \n",
              "3   雖然佢講咗對唔住，但係我都仲係好嬲。  Even though he apologized, I'm still furious.   \n",
              "4               我唯有係等。                               I can only wait.   \n",
              "\n",
              "                                              greedy  \\\n",
              "0                     I'm going to go to the temple.   \n",
              "1  I'm going to say I will give up a fight and go...   \n",
              "2        I'm not a big fan of mysterious characters.   \n",
              "3  I'm not sure if I can do it, but I have a good...   \n",
              "4                             I'm only a single one.   \n",
              "\n",
              "                                                beam  \n",
              "0                  I'm going to go to the monastery.  \n",
              "1  I said I was going to do a series I would give...  \n",
              "2  I don't think I've ever heard of a mysterious ...  \n",
              "3  I'm not sure if we're going to live together, ...  \n",
              "4                     I'm the only one who can wait.  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.to_csv(\"trans_res_Tatoeba_mBart.csv\", index=False)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459848fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_Open_Subtitles.to_csv(\"trans_res_OpenSubtitles_mBart.csv\", index=False)\n",
        "# df_Open_Subtitles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ded1e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_Tatoeba.to_csv(\"trans_res_Tatoeba_Marian.csv\", index=False)\n",
        "df_Tatoeba.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416e67cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_OS_Mar.to_csv(\"trans_res_OpenSubtitles_Marian.csv\", index=False)\n",
        "df_OS_Mar.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
